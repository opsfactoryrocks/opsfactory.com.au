{
    "docs": [
        {
            "location": "/", 
            "text": "Wisdom AWS Guide\n\n\nWelcome to the Ops Factory Wisdom repository for all things AWS. These guides will give you insights into AWS' services and technologies. All the guides are sourced from the \nOpen Guides to AWS\n as well as being provided by us at Ops Factory.\n\n\nThe guides stem from experience, documentation, research \n development, experimentation, and much more. They're an ideal compliment to any engineer's tool set and will hopefully assist in guiding the engineer through the use of AWS services.\n\n\nDisclaimer\n\n\nAs the guides are sourced from a central, public repository which consists of contributions from sources of unknown technical or engineering ability, Ops Factory cannot take responsibility for the quality or accuracy of the information presented here. We will take basic reasonable effort to ensure the information is correct, but no warranty or guarantee is offered.", 
            "title": "Home"
        }, 
        {
            "location": "/#wisdom-aws-guide", 
            "text": "Welcome to the Ops Factory Wisdom repository for all things AWS. These guides will give you insights into AWS' services and technologies. All the guides are sourced from the  Open Guides to AWS  as well as being provided by us at Ops Factory.  The guides stem from experience, documentation, research   development, experimentation, and much more. They're an ideal compliment to any engineer's tool set and will hopefully assist in guiding the engineer through the use of AWS services.", 
            "title": "Wisdom AWS Guide"
        }, 
        {
            "location": "/#disclaimer", 
            "text": "As the guides are sourced from a central, public repository which consists of contributions from sources of unknown technical or engineering ability, Ops Factory cannot take responsibility for the quality or accuracy of the information presented here. We will take basic reasonable effort to ensure the information is correct, but no warranty or guarantee is offered.", 
            "title": "Disclaimer"
        }, 
        {
            "location": "/AMIs/introduction/", 
            "text": "User Guide\n\n\n\n\nUser guide\n\n\n\n\nAMI Basics\n\n\n\n\nAMIs\n (Amazon Machine Images) are immutable images that are used to launch preconfigured EC2 instances. They come in both public and private flavors. Access to public AMIs is either freely available (shared/community AMIs) or bought and sold in the \nAWS Marketplace\n.\n\n\n\n\nVendor AMIs\n\n\n\n\nMany operating system vendors publish ready-to-use base AMIs. For Ubuntu, see the \nUbuntu AMI Finder\n. Amazon of course has \nAMIs for Amazon Linux\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/AMIs/introduction/#user-guide", 
            "text": "User guide", 
            "title": "User Guide"
        }, 
        {
            "location": "/AMIs/introduction/#ami-basics", 
            "text": "AMIs  (Amazon Machine Images) are immutable images that are used to launch preconfigured EC2 instances. They come in both public and private flavors. Access to public AMIs is either freely available (shared/community AMIs) or bought and sold in the  AWS Marketplace .", 
            "title": "AMI Basics"
        }, 
        {
            "location": "/AMIs/introduction/#vendor-amis", 
            "text": "Many operating system vendors publish ready-to-use base AMIs. For Ubuntu, see the  Ubuntu AMI Finder . Amazon of course has  AMIs for Amazon Linux .", 
            "title": "Vendor AMIs"
        }, 
        {
            "location": "/AMIs/tips/", 
            "text": "Correct Deployment Type\n\n\nAMIs are built independently based on how they will be deployed. You must select AMIs that match your deployment when using them or creating them:\n\n\n\n\nEBS or instance store\n\n\nPV or HVM \nvirtualization types\n\n\n32 bit (i386) vs 64 bit (amd64) architecture\n\n\n\n\nAs discussed above, modern deployments will usually be with \n64-bit EBS-backed HVM\n.\n\n\nCustom AMIs\n\n\nYou can create your own custom AMI by \nsnapshotting the state\n of an EC2 instance that you have modified.\n\n\nFaster Time to Standup\n\n\nAMIs backed by EBS storage\n have the necessary image data loaded into the EBS volume itself and don't require an extra pull from S3, which results in EBS-backed instances coming up much faster than instance storage-backed ones.\n\n\nRegion Lock\n\n\nAMIs are \nper region\n, so you must look up AMIs in your region, or copy your AMIs between regions with the \nAMI Copy\n feature.\n\n\naws ec2 copy-image --source-image-id ami-5731123e --source-region us-east-1 --region ap-northeast-1 --name \nMy server\n\n\n\n\n\nSee the \nAWS CLI documentation\n for more details.\n\n\nTag Your AMIs\n\n\nAs with other AWS resources, it's wise to \nuse tags\n to version AMIs and manage their lifecycle.\n\n\nBaking in Ingrediants\n\n\nIf you create your own AMIs, there is always some tension in choosing how much installation and configuration you want to bake into them.\n\n\nMinimal approach\n: baking less into your AMIs (for example, just a configuration management client that downloads, installs, and configures software on new EC2 instances when they are launched) allows you to minimize time spent automating AMI creation and managing the AMI lifecycle (you will likely be able to use fewer AMIs and will probably not need to update them as frequently), but results in longer waits before new instances are ready for use and results in a higher chance of launch-time installation or configuration failures.\n\n\nMiddle of the road\n: baking more into your AMIs (for example, pre-installing but not fully configuring common software along with a configuration management client that loads configuration settings at launch time) results in a faster launch time and fewer opportunities for your software installation and configuration to break at instance launch time but increases the need for you to create and manage a robust AMI creation pipeline.\n\n\nAll in\n: including even more into your AMIs (for example, installing all required software as well and potentially also environment-specific configuration information) results in fast launch times and a much lower chance of instance launch-time failures but (without additional re-deployment and re-configuration considerations) can require time consuming AMI updates in order to update software or configuration as well as more complex AMI creation automation processes.\n\n\nWhich option you favor depends on how quickly you need to scale up capacity, and size and maturity of your team and product.\n\n\nWhen instances boot fast, auto-scaled services require less spare capacity built in and can more quickly scale up in response to sudden increases in load. When setting up a service with autoscaling, consider baking more into your AMIs and backing them with the EBS storage option.\n\n\nAs systems become larger, it's common to have more complex AMI management, such as a multi-stage AMI creation process in which few (ideally one) common base AMIs are infrequently regenerated when components that are common to all deployed services are updated. Then a more frequently run service-level AMI generation process that includes installation and possibly configuration of application-specific software can be utilised.\n\n\nAdditional Reading\n\n\nMore thinking on AMI creation strategies \nhere\n.\n\n\nTooling\n\n\nUse tools like \nPacker\n to simplify and automate AMI creation.", 
            "title": "Tips"
        }, 
        {
            "location": "/AMIs/tips/#correct-deployment-type", 
            "text": "AMIs are built independently based on how they will be deployed. You must select AMIs that match your deployment when using them or creating them:   EBS or instance store  PV or HVM  virtualization types  32 bit (i386) vs 64 bit (amd64) architecture   As discussed above, modern deployments will usually be with  64-bit EBS-backed HVM .", 
            "title": "Correct Deployment Type"
        }, 
        {
            "location": "/AMIs/tips/#custom-amis", 
            "text": "You can create your own custom AMI by  snapshotting the state  of an EC2 instance that you have modified.", 
            "title": "Custom AMIs"
        }, 
        {
            "location": "/AMIs/tips/#faster-time-to-standup", 
            "text": "AMIs backed by EBS storage  have the necessary image data loaded into the EBS volume itself and don't require an extra pull from S3, which results in EBS-backed instances coming up much faster than instance storage-backed ones.", 
            "title": "Faster Time to Standup"
        }, 
        {
            "location": "/AMIs/tips/#region-lock", 
            "text": "AMIs are  per region , so you must look up AMIs in your region, or copy your AMIs between regions with the  AMI Copy  feature.  aws ec2 copy-image --source-image-id ami-5731123e --source-region us-east-1 --region ap-northeast-1 --name  My server   See the  AWS CLI documentation  for more details.", 
            "title": "Region Lock"
        }, 
        {
            "location": "/AMIs/tips/#tag-your-amis", 
            "text": "As with other AWS resources, it's wise to  use tags  to version AMIs and manage their lifecycle.", 
            "title": "Tag Your AMIs"
        }, 
        {
            "location": "/AMIs/tips/#baking-in-ingrediants", 
            "text": "If you create your own AMIs, there is always some tension in choosing how much installation and configuration you want to bake into them.  Minimal approach : baking less into your AMIs (for example, just a configuration management client that downloads, installs, and configures software on new EC2 instances when they are launched) allows you to minimize time spent automating AMI creation and managing the AMI lifecycle (you will likely be able to use fewer AMIs and will probably not need to update them as frequently), but results in longer waits before new instances are ready for use and results in a higher chance of launch-time installation or configuration failures.  Middle of the road : baking more into your AMIs (for example, pre-installing but not fully configuring common software along with a configuration management client that loads configuration settings at launch time) results in a faster launch time and fewer opportunities for your software installation and configuration to break at instance launch time but increases the need for you to create and manage a robust AMI creation pipeline.  All in : including even more into your AMIs (for example, installing all required software as well and potentially also environment-specific configuration information) results in fast launch times and a much lower chance of instance launch-time failures but (without additional re-deployment and re-configuration considerations) can require time consuming AMI updates in order to update software or configuration as well as more complex AMI creation automation processes.  Which option you favor depends on how quickly you need to scale up capacity, and size and maturity of your team and product.  When instances boot fast, auto-scaled services require less spare capacity built in and can more quickly scale up in response to sudden increases in load. When setting up a service with autoscaling, consider baking more into your AMIs and backing them with the EBS storage option.  As systems become larger, it's common to have more complex AMI management, such as a multi-stage AMI creation process in which few (ideally one) common base AMIs are infrequently regenerated when components that are common to all deployed services are updated. Then a more frequently run service-level AMI generation process that includes installation and possibly configuration of application-specific software can be utilised.", 
            "title": "Baking in Ingrediants"
        }, 
        {
            "location": "/AMIs/tips/#additional-reading", 
            "text": "More thinking on AMI creation strategies  here .", 
            "title": "Additional Reading"
        }, 
        {
            "location": "/AMIs/tips/#tooling", 
            "text": "Use tools like  Packer  to simplify and automate AMI creation.", 
            "title": "Tooling"
        }, 
        {
            "location": "/AMIs/traps/", 
            "text": "Amazon Linux Package Versions\n\n\nBy default\n, instances based on Amazon Linux AMIs are configured to point to the latest versions of packages in Amazons package repository. This means that the package versions that get installed are not locked and it is possible for changes, including breaking ones, to appear when applying updates in the future.\n\n\nIf you bake your AMIs with updates already applied, this is unlikely to cause problems in running services whose instances are based on those AMIs. Breaks will appear at the earlier AMI baking stage of your build process, and will need to be fixed or worked around before new AMIs can be generated.\n\n\nLock on Launch\n\n\nThere is a lock on launch feature that allows you to configure Amazon Linux instances to target the repository of a particular major version of the Amazon Linux AMI. This reduces the likelihood that breaks are caused by Amazon-initiated package version changes. The cost of this is not getting the latest packages at the time the EC2 instance is stood up.\n\n\nPairing the use of the lock on launch feature with a process to advance the Amazon Linux AMI at your discretion can give you tighter control over update behaviors and timings.", 
            "title": "Traps"
        }, 
        {
            "location": "/AMIs/traps/#amazon-linux-package-versions", 
            "text": "By default , instances based on Amazon Linux AMIs are configured to point to the latest versions of packages in Amazons package repository. This means that the package versions that get installed are not locked and it is possible for changes, including breaking ones, to appear when applying updates in the future.  If you bake your AMIs with updates already applied, this is unlikely to cause problems in running services whose instances are based on those AMIs. Breaks will appear at the earlier AMI baking stage of your build process, and will need to be fixed or worked around before new AMIs can be generated.", 
            "title": "Amazon Linux Package Versions"
        }, 
        {
            "location": "/AMIs/traps/#lock-on-launch", 
            "text": "There is a lock on launch feature that allows you to configure Amazon Linux instances to target the repository of a particular major version of the Amazon Linux AMI. This reduces the likelihood that breaks are caused by Amazon-initiated package version changes. The cost of this is not getting the latest packages at the time the EC2 instance is stood up.  Pairing the use of the lock on launch feature with a process to advance the Amazon Linux AMI at your discretion can give you tighter control over update behaviors and timings.", 
            "title": "Lock on Launch"
        }, 
        {
            "location": "/API Gateway/alternatives-and-lock-in/", 
            "text": "AWS Gateway could possiblity lock your application into the AWS ecosystem. If this is something of a concern for you, we've provided a self-hosted options below you may be interested.\n\n\nKong\n\n\nKong\n is an open-source, on-premises API and microservices gateway built on nginx with Lua. Kong is extensible through plugins.\n\n\nTyk\n\n\nTyk\n is an open-source API gateway implemented in Go and available in the cloud, on-premises or hybrid.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/API Gateway/alternatives-and-lock-in/#kong", 
            "text": "Kong  is an open-source, on-premises API and microservices gateway built on nginx with Lua. Kong is extensible through plugins.", 
            "title": "Kong"
        }, 
        {
            "location": "/API Gateway/alternatives-and-lock-in/#tyk", 
            "text": "Tyk  is an open-source API gateway implemented in Go and available in the cloud, on-premises or hybrid.", 
            "title": "Tyk"
        }, 
        {
            "location": "/API Gateway/development/", 
            "text": "Developing against API Gateway can be somewhat tedious, especially if you're creating and managing the Lambda functions and Gateway directly via the Console or CLI tools. To make things easier, we've included below a few frameworks you can use to rapidly speed up the development and deployment of API Gateway based applications.\n\n\nZappa\n\n\nZappa\n - \"Zappa makes it super easy to deploy all Python WSGI applications on AWS Lambda + API Gateway. Think of it as \"serverless\" web hosting for your Python web apps. That means infinite scaling, zero downtime, zero maintenance - and at a fraction of the cost of your current deployments!\"\n\n\nChalice\n\n\nChalice\n - \"The python serverless microframework for AWS allows you to quickly create and deploy applications that use Amazon API Gateway and AWS Lambda.\"", 
            "title": "Development"
        }, 
        {
            "location": "/API Gateway/development/#zappa", 
            "text": "Zappa  - \"Zappa makes it super easy to deploy all Python WSGI applications on AWS Lambda + API Gateway. Think of it as \"serverless\" web hosting for your Python web apps. That means infinite scaling, zero downtime, zero maintenance - and at a fraction of the cost of your current deployments!\"", 
            "title": "Zappa"
        }, 
        {
            "location": "/API Gateway/development/#chalice", 
            "text": "Chalice  - \"The python serverless microframework for AWS allows you to quickly create and deploy applications that use Amazon API Gateway and AWS Lambda.\"", 
            "title": "Chalice"
        }, 
        {
            "location": "/API Gateway/gotchas-and-limitations/", 
            "text": "API Gateway only supports encrypted (https) endpoints, and does not support unencrypted HTTP. (This is probably a good thing.)\n\n\nAPI Gateway endpoints are always public, i.e. internet facing, and there is no mechanism to build private endpoints, e.g. for internal use on a \nVPC\n but endpoints and their related resources can, optionally, \nrequire authentication\n.\n\n\nAPI Gateway doesnt support multi-region deployments for high availability. It is a service that is deployed in a single region but comes with a global endpoint that is served from AWS edge locations (similar to a CloudFront distribution). You cannot have multiple API Gateways with the same hostname in different AWS regions and use Route 53 to distribute the traffic. More in \nthis forum post\n.\n\n\nIntegration timeout: All of the various integration types (eg: Lambda, HTTP) for API Gateway have timeouts, as described \nhere\n. Unlike some limits, these timeouts can't be increased.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/API Gateway/introduction/", 
            "text": "Homepage\n\n\nHomepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nBrief\n\n\nAPI Gateway provides a scalable, secured front-end for service APIs, and can work with Lambda, Elastic Beanstalk, or regular EC2 services. It allows serverless deployment of applications built with Lambda.\n\n\nHowever, switching over deployments after upgrades can be tricky. There are no built-in mechanisms to have a single domain name migrate from one API gateway to another one. So it may be necessary to build an additional layer in front (even another API Gateway) to allow smooth migration from one deployment to another.", 
            "title": "Introduction"
        }, 
        {
            "location": "/API Gateway/introduction/#homepage", 
            "text": "Homepage    Developer guide    FAQ    Pricing", 
            "title": "Homepage"
        }, 
        {
            "location": "/API Gateway/introduction/#brief", 
            "text": "API Gateway provides a scalable, secured front-end for service APIs, and can work with Lambda, Elastic Beanstalk, or regular EC2 services. It allows serverless deployment of applications built with Lambda.  However, switching over deployments after upgrades can be tricky. There are no built-in mechanisms to have a single domain name migrate from one API gateway to another one. So it may be necessary to build an additional layer in front (even another API Gateway) to allow smooth migration from one deployment to another.", 
            "title": "Brief"
        }, 
        {
            "location": "/Application Load Balancer/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nWebsockets and HTTP/2\n are \nnow supported\n.\n\n\nInternet Protocol Version 6 (IPv6)\n is \nnow supported\n.\n\n\nPrior to the Application Load Balancer, you were advised to use TCP instead of HTTP as the protocol to make it work (as described \nhere\n) and use \nthe obscure but useful Proxy Protocol\n (\nmore on this\n) to pass client IPs over a TCP load balancer.", 
            "title": "Basics"
        }, 
        {
            "location": "/Application Load Balancer/gotchas-and-limitations/", 
            "text": "ALBs only support HTTP/2 over HTTPS (no plain-text HTTP/2).\n\n\nALBs only support HTTP/2 to external clients and not to internal resources (instances/containers).\n\n\nALBs support HTTP routing but not port-based TCP routing.\n\n\nALBs do not (yet) support routing based on HTTP Host header or HTTP verb.\n\n\nInstances in the ALBs target groups have to either have a single, fixed healthcheck port (EC2 instance-level healthcheck) or the healthcheck port for a target has to be the same as its application port (Application instance-level healthcheck) - you can't configure a per-target healthcheck port that is different than the application port.\n\n\nALBs are VPC-only (they are not available in EC2 Classic)\n\n\nIn a target group, if there is no healthy target, all requests are routed to all targets. For example, if you point a listener at a target group containing a single service that has a long initialization phase (during which the health checks would fail), requests will reach the service while it is still starting up.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Application Load Balancer/tips/", 
            "text": "Use ALBs to route to services that are hosted on shared clusters with dynamic port assignment (like ECS or Mesos).\n\n\nALBs support HTTP path-based routing (send HTTP requests for /api/\n -\n {target-group-1}, /blog/\n -\n {target group 2}).", 
            "title": "Tips"
        }, 
        {
            "location": "/Aurora/basics/", 
            "text": "Amazons proprietary fork of MySQL intended to scale up for high concurrency workloads. Generally speaking, individual query performance under Aurora is not expected to improve significantly relative to MySQL or MariaDB, but Aurora is intended to maintain performance while executing many more queries concurrently than an equivalent MySQL or MariaDB server could handle.\n\n\nNotable new features\n include:\n\n\nLog-structured storage instead of B-trees to improve write performance\n\n\nOut-of-process buffer pool so that databases instances can be restarted without clearing the buffer pool\n\n\nThe underlying physical storage is a specialized SSD array that automatically maintains 6 copies of your data across 3 AZs.\n\n\nAurora read replicas share the storage layer with the write master which significantly reduces replica lag, eliminates the need for the master to write and distribute the binary log for replication, and allows for zero-data-loss failovers from the master to a replica. The master and all the read replicas that share storage are known collectively as an \nAurora cluster\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/Aurora/gotchas-and-limitations/", 
            "text": "Aurora is based on MySQL 5.6.10\n with some cherry-picking of later MySQL features. It is missing most 5.7 features as well as some online DDL features introduced in 5.6.17.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Aurora/tips/", 
            "text": "In order to take advantage of Auroras higher concurrency, applications should be configured with large database connection pools and should execute as many queries concurrently as possible. For example, Aurora servers have been tested to produce increasing performance on some OLTP workloads with \nup to 5,000 connections\n.\n\n\nAurora scales well with multiple CPUs\n and may require a large instance class for optimal performance.\n\n\nBecause Aurora is based on MySQL 5.6.10, avoiding any MySQL features from 5.7 or later will ease the transition from a MySQL-compatible database into Aurora.\n\n\nThe easiest migration path to Aurora is restoring a database snapshot from MySQL 5.6. The next easiest method is restoring a dump from a MySQL-compatible database such as MariaDB. For \nlow-downtime migrations\n from other MySQL-compatible databases, you can set up an Aurora instance as a replica of your existing database. If none of those methods are options, Amazon offers a fee-based data migration service.\n\n\nYou can replicate \nfrom an Aurora cluster to MySQL or to another Aurora cluster\n. This requires binary logging to be enabled and is not as performant as native Aurora replication.", 
            "title": "Tips"
        }, 
        {
            "location": "/Auto Scaling/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n at no additional charge\n\n\nAuto Scaling Groups (ASGs)\n are used to control the number of instances in a service, reducing manual effort to provision or deprovision EC2 instances.\n\n\nThey can be configured through \nScaling Policies\n to automatically increase or decrease instance counts based on metrics like CPU utilization, or based on a schedule.\n\n\nThere are three common ways of using ASGs - dynamic (automatically adjust instance count based on metrics for things like CPU utilization), static (maintain a specific instance count at all times), scheduled (maintain different instance counts at different times of day or on days of the week).\n\n\nASGs \nhave no additional charge\n themselves; you pay for underlying EC2 and CloudWatch services.", 
            "title": "Basics"
        }, 
        {
            "location": "/Auto Scaling/gotchas-and-limitations/", 
            "text": "ReplaceUnhealthy setting:\n By default, ASGs will kill instances that the EC2 instance manager considers to be unresponsive. It is possible for instances whose CPU is completely saturated for minutes at a time to appear to be unresponsive, causing an ASG with the default \nReplaceUnhealthy setting\n turned on to replace them. When instances that are managed by ASGs are expected to consistently run with very high CPU, consider deactivating this setting. If you do so, however, detecting and killing unhealthy nodes will become your responsibility.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Auto Scaling/tips/", 
            "text": "Better matching your cluster size to your current resource requirements through use of ASGs can result in significant cost savings for many types of workloads.\n\n\nPairing ASGs with CLBs is a common pattern used to deal with changes in the amount of traffic a service receives.\n\n\nDynamic Auto Scaling is easiest to use with stateless, horizontally scalable services.\n\n\nEven if you are not using ASGs to dynamically increase or decrease instance counts, you should seriously consider maintaining all instances inside of ASGs  given a target instance count, the ASG will work to ensure that number of instances running is equal to that target, replacing instances for you if they die or are marked as being unhealthy. This results in consistent capacity and better stability for your service.\n\n\nAutoscalers can be \nconfigured to terminate\n instances that a CLB or ALB has marked as being unhealthy.", 
            "title": "Tips"
        }, 
        {
            "location": "/Billing/basics/", 
            "text": "AWS offers a \nfree tier\n of service, that allows very limited usage of resources at no cost. For example, a micro instance and small amount of storage is available for no charge. (If you have an old account but starting fresh, sign up for a new one to qualify for the free tier.) \nAWS Activate\n extends this to tens of thousands of dollars of free credits to startups in \ncertain funds or accelerators\n.\n\n\nYou can set \nbilling alerts\n to be notified of unexpected costs, such as costs exceeding the free tier. You can set these in a \ngranular way\n.\n\n\nAWS offers \nCost Explorer\n, a tool to get better visibility into costs.\n\n\nUnfortunately, the AWS console and billing tools are rarely enough to give good visibility into costs. For large accounts, the AWS billing console can time out or be too slow to use.\n\n\nTools:\n\n\nEnable \nbilling reports\n and install an open source tool to help manage or monitor AWS resource utilization. \nNetflix Ice\n is probably the first one you should try. Check out \ndocker-ice\n for a Dockerized version that eases installation.\n\n\nOne challenge with Ice is that it doesnt cover amortized cost of reserved instances.\n\n\nOther tools include \nSecurity Monkey\n and \nCloud Custodian\n.\n\n\n\n\n\n\nThird-party services:\n Several companies offer services designed to help you gain insights into expenses or lower your AWS bill, such as \nOpsClarity\n, \nCloudability\n, \nCloudHealth Technologies\n, and \nParkMyCloud\n. Some of these charge a percentage of your bill, which may be expensive. See the \nmarket landscape\n.\n\n\nAWSs \nTrusted Advisor\n is another service that can help with cost concerns.\n\n\nDont be shy about asking your account manager for guidance in reducing your bill. Its their job to keep you happily using AWS.\n\n\nTagging for cost visibility:\n As the infrastructure grows, a key part of managing costs is understanding where they lie. Its strongly advisable to \ntag resources\n, and as complexity grows, group them effectively. If you \nset up billing allocation appropriately\n, you can then get visibility into expenses according to organization, product, individual engineer, or any other way that is helpful.\n\n\nIf you need to do custom analysis of raw billing data or want to feed it to a third party cost analysis service, \nenable\n the \ndetailed billing report\n feature.\n\n\nMultiple Amazon accounts can be linked for billing purposes using the \nConsolidated Billing\n feature. Large enterprises may need complex billing structures depending on ownership and approval processes.", 
            "title": "Basics"
        }, 
        {
            "location": "/Billing/data-transfer-costs/", 
            "text": "For deployments that involve significant network traffic, a large fraction of AWS expenses are around data transfer. Furthermore, costs of data transfer, within AZs, within regions, between regions, and into and out of AWS and the internet vary significantly depending on deployment choices.\n\n\nSome of the most common gotchas:\n\n\nAZ-to-AZ traffic:\n Note EC2 traffic between AZs is effectively the same as between regions. For example, deploying a Cassandra cluster across AZs is helpful for \nhigh availability\n, but can hurt on network costs.\n\n\nUsing public IPs when not necessary:\n If you use an Elastic IP or public IP address of an EC2 instance, you will incur network costs, even if it is accessed locally within the AZ.\n\n\n\n\n\n\nThis figure gives an overview:", 
            "title": "Data transfer costs"
        }, 
        {
            "location": "/Billing/ec2-cost-management/", 
            "text": "With EC2, there is a trade-off between engineering effort (more analysis, more tools, more complex architectures) and spend rate on AWS. If your EC2 costs are small, many of the efforts here are not worth the engineering time required to make them work. But once you know your costs will be growing in excess of an engineers salary, serious investment is often worthwhile.\n\n\nLarger instances arent necessarily priced higher in the spot market  therefore, you should look at the available options and determine which instances will be most cost effective for your jobs. See \nBid Advisor\n.\n\n\nSpot instances:\n\n\nEC2 \nSpot instances\n are a way to get EC2 resources at significant discount  often many times cheaper than standard on-demand prices  if youre willing to accept the possibility that they be terminated with little to no warning.\n\n\nUse Spot instances for potentially very significant discounts whenever you can use resources that may be restarted and dont maintain long-term state.\n\n\nThe huge savings that you can get with Spot come at the cost of a significant increase in complexity when provisioning and reasoning about the availability of compute capacity.\n\n\nAmazon maintains Spot prices at a market-driven fluctuating level, based on their inventory of unused capacity. Prices are typically low but can \nspike\n very high. See the \nprice history\n to get a sense for this.\n\n\nYou set a bid price high to indicate how high youre willing to pay, but you only pay the going rate, not the bid rate. If the market rate exceeds the bid, your instance may be terminated.\n\n\nPrices are per instance type and per availability zone. The same instance type may have wildly different price in different zones at the same time. Different instance types can have very different prices, even for similarly powered instance types in the same zone.\n\n\nCompare prices across instance types for better deals.\n\n\nUse Spot instances whenever possible. Setting a high bid price will assure your machines stay up the vast majority of the time, at a fraction of the price of normal instances.\n\n\nGet notified up to two minutes before price-triggered shutdown by polling \nyour Spot instances metadata\n.\n\n\nMake sure your usage profile works well for Spot before investing heavily in tools to manage a particular configuration.\n\n\n\n\n\n\nSpot fleet:\n\n\nYou can realize even bigger cost reductions at the same time as improvements to fleet stability relative to regular Spot usage by using \nSpot fleet\n to bid on instances across instance types, availability zones, and (through multiple Spot Fleet Requests) regions.\n\n\nSpot fleet targets maintaining a specified (and weighted-by-instance-type) total capacity across a cluster of servers. If the Spot price of one instance type and availability zone combination rises above the weighted bid, it will rotate running instances out and bring up new ones of another type and location up in order to maintain the target capacity without going over target cluster cost.\n\n\n\n\n\n\nSpot usage best practices:\n\n\nApplication profiling:\n\n\nProfile your application to figure out its runtime characteristics. That would help give an understanding of the minimum cpu, memory, disk required. Having this information is critical before you try to optimize spot costs.\n\n\nOnce you know the minimum application requirements, instead of resorting to fixed instance types, you can bid across a variety of instance types (that gives you higher chances of getting a spot instance to run your application).E.g., If you know that 4 cpu cores are enough for your job, you can choose any instance type that is equal or above 4 cores and that has the least Spot price based on history. This helps you bid for instances with greater discount (less demand at that point).\n\n\n\n\n\n\nSpot price monitoring and intelligence:\n\n\nSpot Instance prices fluctuate depending on instance types, time of day, region and availability zone. The AWS CLI tools and API allow you to describe Spot price metadata given time, instance type, and region/AZ.\n\n\nBased on history of Spot instance prices, you could potentially build a myriad of algorithms that would help you to pick an instance type in a way that \noptimizes cost\n, \nmaximizes availability\n, or \noffers predictable performance\n.\n\n\nYou can also track the number of times an instance of certain type got taken away (out bid) and plot that in graphite to improve your algorithm based on time of day.\n\n\n\n\n\n\nSpot machine resource utilization:\n\n\nFor running spiky workloads (spark, map reduce jobs) that are schedule based and where failure is non critical, Spot instances become the perfect candidates.\n\n\nThe time it takes to satisfy a Spot instance could vary between 2-10 mins depending on the type of instance and availability of machines in that AZ.\n\n\nIf you are running an infrastructure with hundreds of jobs of spiky nature, it is advisable to start pooling instances to optimize for cost, performance and most importantly time to acquire an instance.\n\n\nPooling implies creating and maintaining Spot instances so that they do not get terminated after use. This promotes re-use of Spot instances across jobs. This of course comes with the overhead of lifecycle management.\n\n\nPooling has its own set of metrics that can be tracked to optimize resource utilization, efficiency and cost.\n\n\nTypical pooling implementations give anywhere between 45-60% cost optimizations and 40% reduction in spot instance creation time.\n\n\nAn excellent example of Pooling implementation described by Netflix (\npart1\n, \npart2\n)\n\n\n\n\n\n\n\n\n\n\nSpot management gotchas\n\n\nLifetime:\n There is \nno guarantee\n for the lifetime of a Spot instance. It is purely based on bidding. If anyone outbids your price, the instance is taken away. Spot is not suitable for time sensitive jobs that have strong SLA. Instances will fail based on demand for Spot at that time. AWS provides a \ntwo-minute warning\n before Amazon EC2 must terminate your Spot instance.\n\n\nAPI return data:\n - The Spot price API returns Spot prices of varying granularity depending on the time range specified in the api call.E.g If the last 10 min worth of history is requested, the data is more fine grained. If the last 2 day worth of history is requested, the data is more coarser. Do not assume you will get all the data points. There \nwill\n be skipped intervals.\n\n\nLifecycle management:\n Do not attempt any fancy Spot management unless absolutely necessary. If your entire usage is only a few machines and your cost is acceptable and your failure rate is lower, do not attempt to optimize. The pain for building/maintaining it is not worth just a few hundred dollar savings.\n\n\n\n\n\n\nReserved Instances:\n allow you to get significant discounts on EC2 compute hours in return for a commitment to pay for instance hours of a specific instance type in a specific AWS region and availability zone for a pre-established time frame (1 or 3 years). Further discounts can be realized through partial or all upfront payment options.\n\n\nConsider using Reserved Instances when you can predict your longer-term compute needs and need a stronger guarantee of compute availability and continuity than the (typically cheaper) Spot market can provide. However be aware that if your architecture changes your computing needs may change as well so long term contracts can seem attractive but may turn out to be cumbersome.\n\n\nThere are two types of Reserved Instances - \nStandard and Convertible\n. If you purchase excess Standard Reserved Instances, you may offer to sell back unused Reserved Instances via the \nReserved Instance Marketplace\n, this allows you to potentially recoup the cost of unused EC2 compute instance hours by selling them to other AWS customers.\n\n\nInstance reservations are not tied to specific EC2 instances - they are applied at the billing level to eligible compute hours as they are consumed across all of the instances in an account.\n\n\n\n\n\n\nHourly billing waste:\n EC2 instances are \nbilled in instance-hours\n  rounded up to the nearest full hour! For long-lived instances, this is not a big worry, but for large transient deployments, like EMR jobs or test deployments, this can be a significant expense. Never deploy many instances and terminate them after only a few minutes. In fact, if transient instances are part of your regular processing workflow, you should put in protections or alerts to check for this kind of waste.\n\n\nIf you have multiple AWS accounts and have configured them to roll charges up to one account using the Consolidated Billing feature, you can expect \nunused\n Reserved Instance hours from one account to be applied to matching (region, availability zone, instance type) compute hours from another account.\n\n\nIf you have multiple AWS accounts that are linked with Consolidated Billing, plan on using reservations, and want unused reservation capacity to be able to apply to compute hours from other accounts, youll need to create your instances in the availability zone with the same \nname\n across accounts. Keep in mind that when you have done this, your instances may not end up in the same \nphysical\n data center across accounts - Amazon shuffles availability zones names across accounts in order to equalize resource utilization.\n\n\nMake use of dynamic \nAuto Scaling\n, where possible, in order to better match your cluster size (and cost) to the current resource requirements of your service.", 
            "title": "Ec2 cost management"
        }, 
        {
            "location": "/Certificate Manager/alternatives-and-lock-in/", 
            "text": "Certificates issued by the Certificate Manager cant be used outside of the services that support it. Imported certificates, however, can still be used elsewhere.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/Certificate Manager/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nUse the \nCertificate Manager\n to manage SSL/TLS certificates in other AWS services.\n\n\nSupports importing existing certificates as well as issuing new ones.", 
            "title": "Basics"
        }, 
        {
            "location": "/Certificate Manager/gotchas-and-limitations/", 
            "text": "In order to use \nCertificate Manager\n for CloudFront distributions certificate must be issued or imported from us-east-1 (N. Virginia) region. Certificates from other regions can \nonly be used with Elastic Load Balancers\n.\n\n\nIoT\n has its \nown way\n of setting up certificates.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Certificate Manager/tips/", 
            "text": "Supported services:\n Managed \nLoad Balancers\n and \nCloudFront\n.\n\n\nDuring the domain validation process, Certificate Manager will send an email to every contact address specified in the domains WHOIS record and up to five common administrative addresses. Some anti-spam filters can mark emails as spam because of this. You should check the spam folder of your email if you dont receive a confirmation email.", 
            "title": "Tips"
        }, 
        {
            "location": "/CloudFormation/alternatives-and-lock-in/", 
            "text": "Hashicorps \nTerraform\n is a third-party alternative that can support other cloud platforms/providers including \nAzure\n and \nOpenStack\n.\n\n\nSome AWS features may not be available in Terraform (e.g. multi-AZ ElastiCache using Redis), and you may have to resort to embedded CloudFormation templates.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/CloudFormation/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n at no additional charge\n\n\nCloudFormation\n offers mechanisms to create and update entire \nstacks\n comprised of many types of AWS resources.  These CloudFormation stacks are defined in a \nCloudFormation template\n which is defined in \nJSON\n or \nYAML\n.\n\n\nCloudFormation itself has \nno additional charge\n itself; you pay for the underlying resources.", 
            "title": "Basics"
        }, 
        {
            "location": "/CloudFormation/gotchas-and-limitations/", 
            "text": "Modifications to stack resources made outside CloudFormation can potentially lead to stacks stuck in UPDATE_ROLLBACK_FAILED mode. Stacks in this state cant be recovered without help from AWS Support.\n\n\nCloudFormation is useful but complex and with a variety of pain points. Many companies find alternate solutions, and many companies use it, but only with significant additional tooling.\n\n\nCloudFormation can be very slow, especially for items like CloudFront distributions and Route53 CNAME entries.\n\n\nIts hard to assemble good CloudFormation configurations from existing state. AWS does \noffer a trick to do this\n, but its very clumsy.\n\n\nMany users dont use CloudFormation at all because of its limitations, or because they find other solutions preferable. Often there are other ways to accomplish the same goals, such as local scripts (Boto, Bash, Ansible, etc.) you manage yourself that build infrastructure, or Docker-based solutions (\nConvox\n, etc.).", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/CloudFormation/tips/", 
            "text": "Troposphere\n is a Python library that makes it much easier to create CloudFormation templates.\n\n\nCurrently supports \nAWS\n and \nOpenStack\n resource types.\n\n\nTroposphere does not support all of the resources types you can describe with CloudFormation templates.\n\n\nBuilt in \nerror\n checking.\n\n\nA recommended soft dependency is \nawacs\n, which allows you to generate AWS access policy in JSON by writing Python code.\n\n\n\n\n\n\nIf you are building different stacks with similar layers, it may be useful to build separate templates for each layer that you can reuse using \nAWS::CloudFormation::Stack\n.\n\n\nAvoid hardcoding resource parameters that can potentially change. Use stack parameters as much as you can, and resort to default parameter values.\n\n\nUntil \n2016\n, CloudFormation used only an awkward JSON format that makes both reading and debugging difficult. To use it effectively typically involved building additional tooling, including converting it to YAML, but now \nthis is supported directly\n.\n\n\nWherever possible, export relevant \nphysical IDs\n from your Stacks by defining \nOutputs in your CloudFormation Templates\n. These are the actual names assigned to the resources being created. Outputs can be returned from \nDescribeStack\n API calls, and get imported to other Stacks as part of the \nrecent addition\n of \ncross-stack references\n.\n\n\nCloudFormation can be set up to \nsend SNS notifications\n upon state changes, enabling programatic handling of situations where stacks fail to build, or simple email alerts so the appropriate people are informed.\n\n\nCloudFormation allows the use of \nconditionals\n when creating a stack.\n\n\nOne common way to leverage this capability is in support of multi-environment CloudFormation templates  by configuring them to use if-else statements on the value of a \nparameter passed in\n (e.g.  env), environment-specific values for things like VPC IDs, SecurityGroup IDs, and AMI names can be passed into reusable generic templates.\n\n\n\n\n\n\nVersion control your CloudFormation templates!\n In the Cloud, an application is the combination of the code written and the infrastructure it runs on. By version controlling \nboth\n, it is easy to roll back to known good states.", 
            "title": "Tips"
        }, 
        {
            "location": "/CloudFront/alternatives-and-lock-in/", 
            "text": "CDNs are \na highly fragmented market\n. CloudFront has grown to be a leader, but there are many alternatives that might better suit specific needs.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/CloudFront/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nCloudFront\n is AWS \ncontent delivery network (CDN)\n.\n\n\nIts primary use is improving latency for end users through accessing cacheable content by hosting it at \nover 60 global edge locations\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/CloudFront/gotchas-and-limitations/", 
            "text": "If using S3 as a backing store, remember that the endpoints for website hosting and for general S3 are different. Example: bucketname.s3.amazonaws.com is a standard S3 serving endpoint, but to have redirect and error page support, you need to use the website hosting endpoint listed for that bucket, e.g. bucketname.s3-website-us-east-1.amazonaws.com (or the appropriate region).\n\n\nBy default, CloudFront will not forward HTTP Host: headers through to your origin servers. This can be problematic for your origin if you run multiple sites switched with host headers. You can \nenable host header forwarding\n in the default cache behavior settings.\n\n\n4096-bit SSL certificates: CloudFront do not support 4096-bit SSL certificates as of late 2016. If you are using an externally issued SSL certificate, youll need to make sure its 2048 bits. See \nongoing discussion\n.\n\n\nAlthough connections from clients to CloudFront edge servers can make use of IPv6, \nconnections to the origin server will continue to use IPv4.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/CloudFront/tips/", 
            "text": "IPv6\n is \nsupported\n. This is a configurable setting, and is enabled by default on new CloudFront distributions. IPv6 support extends to the use of WAF with CloudFront.\n\n\nHTTP/2\n is \nnow supported\n! Clients \nmust support TLS 1.2 and SNI\n.\n\n\nWhile the most common use is for users to browse and download content (GET or HEAD methods) requests, CloudFront also supports (\nsince 2013\n) uploaded data (POST, PUT, DELETE, OPTIONS, and PATCH).\n\n\nYou must enable this by specifying the \nallowed HTTP methods\n when you create the distribution.\n\n\nInterestingly, the cost of accepting (uploaded) data \nis usually less\n than for sending (downloaded) data.\n\n\n\n\n\n\nIn its basic version, CloudFront \nsupports SSL\n via the \nSNI extension to TLS\n, which is supported by all modern web browsers. If you need to support older browsers, you need to pay a few hundred dollars a month for dedicated IPs.\n\n\nConsider invalidation needs carefully. CloudFront \ndoes support invalidation\n of objects from edge locations, but this typically takes many minutes to propagate to edge locations, and costs $0.005 per request after the first 1000 requests. (Some other CDNs support this better.)\n\n\n\n\n\n\nEveryone should use TLS nowadays if possible. \nIlya Grigoriks table\n offers a good summary of features regarding TLS performance features of CloudFront.\n\n\nAn alternative to invalidation that is often easier to manage, and instant, is to configure the distribution to \ncache with query strings\n and then append unique query strings with versions onto assets that are updated frequently.\n\n\nFor good web performance, it is recommended to \nenable compression\n on CloudFront distributions if the origin is S3 or another source that does not already compress.", 
            "title": "Tips"
        }, 
        {
            "location": "/CloudWatch/alternatives-and-lock-in/", 
            "text": "CloudWatch offers fairly basic functionality that doesn't create significant (additional) AWS lock-in. Most of the metrics provided by the service can be obtained through APIs that can be imported into other aggregation or visualization tools or services (many specifically provide CloudWatch data import services).\n\n\nAlternatives to CloudWatch monitoring services include \nNewRelic\n, \nDatadog\n, \nSumo Logic\n, \nZabbix\n, \nNagios\n, \nRuxit\n, \nElastic Stack\n, open source options such as \nStatsD\n or \ncollectd\n with \nGraphite\n, and many others.\n\n\nCloudWatch Log alternatives include \nSplunk\n, \nSumo Logic\n, \nLoggly\n, \nLogstash\n, \nPapertrail\n, \nElastic Stack\n, and other centralized logging solutions.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/CloudWatch/basics/", 
            "text": "Homepage\n  \nDocumentation\n  \nFAQ\n  \nPricing\n\n\nCloudWatch\n monitors resources and applications, captures logs, and sends events.\n\n\nCloudWatch monitoring is the standard mechanism for keeping tabs on AWS resources. A wide range of  \nmetrics and dimensions\n are available via CloudWatch, allowing you to create time based graphs, \nalarms\n, and \ndashboards\n.\n\n\nAlarms are the most practical use of CloudWatch, allowing you to trigger notifications from any given metric.\n\n\nAlarms can trigger \nSNS notifications\n, \nAuto Scaling actions\n, or \nEC2 actions\n.\n\n\nPublish and share graphs of metrics by creating \ncustomizable dashboard views\n.\n\n\nMonitor and report on EC2 \ninstance system check failure alarms\n.\n\n\n\n\n\n\n\n\n\n\nUsing CloudWatch Events:\n\n\nEvents create a mechanism to automate actions in various services on AWS. You can create \nevent rules\n from instance states, AWS APIs, Auto Scaling, Run commands, deployments or time-based schedules (think Cron).\n\n\nTriggered events\n can can invoke Lambda functions, send SNS/SQS/Kinesis messages, or perform instance actions (terminate, restart, stop, or snapshot volumes).\n\n\nCustom payloads can be sent to targets in JSON format, this is especially useful when triggering Lambdas.\n\n\n\n\n\n\nUsing CloudWatch Logs:\n\n\nCloudWatch Logs\n is a streaming log storage system. By storing logs within AWS you have access to unlimited paid storage, but you also have the option of streaming logs directly to ElasticSearch or custom Lambdas.\n\n\nA \nlog agent installed\n on your servers will process logs over time and send them to CloudWatch Logs.\n\n\nYou can \nexport logged data to S3\n or stream results to other AWS services.\n\n\n\n\n\n\nDetailed monitoring:\n \nDetailed monitoring\n for EC2 instances must be enabled to get granular metrics, and is \nbilled under CloudWatch\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/CloudWatch/gotchas-and-limitations/", 
            "text": "Metrics in CloudWatch originate \non the hypervisor\n. The hypervisor doesn't have access to OS information, so certain metrics (most notably memory utilization) are not available unless pushed to CloudWatch from inside the instance.\n\n\nYou can not use \nmore than one metric for an alarm\n.\n\n\nNotifications you receive from alarms will not have any contextual detail; they have only the specifics of the threshold, alarm state, and timing.\n\n\nMinimum granularity in CloudWatch is 1 minute. That means that multiple values of a metric that are pushed to CloudWatch within the same minute are aggregated into minimum, maximum, average and total (sum) per minute.\n\n\nData about metrics is kept in CloudWatch \nfor 15 months\n, starting November 2016 (used to be 14 days). Minimum granularity increases after 15 days.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/CloudWatch/tips/", 
            "text": "Some very common use cases for CloudWatch are \nbilling alarms\n, \ninstance\n \nor \nload balancer up/down alarms\n, and \ndisk usage alerts\n.\n\n\nYou can use \nEC2Config\n to monitor watch memory and disk metrics on Windows platform instances. For Linux, there are \nexample scripts\n that do the same thing.\n\n\nYou can \npublish your own metrics\n using the AWS API. \nIncurs additional cost\n.\n\n\nYou can stream directly from CloudWatch Logs to a Lambda or ElasticSearch cluster by creating \nsubscriptions\n on Log Groups.\n\n\nDon't forget to take advantage of the \nCloudWatch non-expiring free tier\n.", 
            "title": "Tips"
        }, 
        {
            "location": "/Device Farm/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nDevice Farm\n is an AWS service that enables mobile app testing on real devices.\n\n\nSupports iOS and Android (including Kindle Fire) devices, as well as the mobile web.\n\n\nSupports remote device access in order to allow for interactive testing/debugging.", 
            "title": "Basics"
        }, 
        {
            "location": "/Device Farm/gotchas-and-limitations/", 
            "text": "Devices don't have a SIM card and therefore cant be used for testing SIM card-related features.\n\n\nDevice Farm supports testing for most popular languages/frameworks, but not for all. An actual list of supported frameworks and languages is presented on \nthis page\n.\n\n\nThe API and CLI for Device Farm is quite a low level and may require developing additional tools or scripts on top of it.\n\n\nAWS provide several tools and plugins for Device Farm, however, it doesnt cover all cases or platforms. It may require developing specific tools or plugins to support specific requirements.\n\n\nIn general, Device Farm doesnt have Android devices from Chinese companies like Huawei, Meizu, Lenovo, etc. An actual list of supported devices located \nhere\n.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Device Farm/tips/", 
            "text": "AWS Mobile blog\n contains several examples of Device Farm usage for testing.\n\n\nDevice Farm offers a free trial for users who want to evaluate their service.\n\n\nDevice Farm offers two pricing models: Paying \nper device minute\n is useful for small usage levels or for situations where its hard to predict usage amount. \nUnmetered plans\n are useful in situations where active usage is expected from the beginning.", 
            "title": "Tips"
        }, 
        {
            "location": "/DirectConnect/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nDirect Connect\n is a private, dedicated connection from your network(s) to AWS.", 
            "title": "Basics"
        }, 
        {
            "location": "/DirectConnect/tips/", 
            "text": "If your data center has \na partnering relationship\n with AWS, setup is streamlined.\n\n\nUse for more consistent predictable network performance guarantees (\n1 Gbps\n or \n10 Gbps\n per link).\n\n\nUse to peer your colocation, corporate, or physical datacenter network with your VPC(s).\n\n\nExample: Extend corporate LDAP and/or Kerberos to EC2 instances running in a VPC.\n\n\nExample: Make services that are hosted outside of AWS for financial, regulatory, or legacy reasons callable from within a VPC.", 
            "title": "Tips"
        }, 
        {
            "location": "/DynamoDB/alternatives-and-lock-in/", 
            "text": "Unlike the technologies behind many other Amazon products, DynamoDB is a proprietary AWS product with no interface-compatible alternative available as an open source project. If you tightly couple your application to its API and featureset, it will take significant effort to replace.\n\n\nThe most commonly used alternative to DynamoDB is \nCassandra\n.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/DynamoDB/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nDynamoDB\n is a \nNoSQL\n database with focuses on speed, flexibility, and scalability.\n\n\nDynamoDB is priced on a combination of throughput and storage.", 
            "title": "Basics"
        }, 
        {
            "location": "/DynamoDB/gotchas-and-limitations/", 
            "text": "DynamoDB doesnt provide an easy way to bulk-load data (it is possible through \nData Pipeline\n) and this has some \nunfortunate consequences\n. Since you need to use the regular service APIs to update existing or create new rows, it is common to temporarily turn up a destination tables write throughput to speed import. But when the tables write capacity is increased, DynamoDB may do an irreversible split of the partitions underlying the table, spreading the total table capacity evenly across the new generation of tables. Later, if the capacity is reduced, the capacity for each partition is also reduced but the total number of partitions is not, leaving less capacity for each partition. This leaves the table in a state where it much easier for hotspots to overwhelm individual partitions.\n\n\nIt is important to make sure that DynamoDB \nresource limits\n are compatible with your dataset and workload. For example, the maximum size value that can be added to a DynamoDB table is 400 KB (larger items can be stored in S3 and a URL stored in DynamoDB).\n\n\nDealing with \ntime series data\n in DynamoDB can be challenging. A global secondary index together with down sampling timestamps can be a possible solution as explained \nhere\n.\n\n\nDynamoDB does \nnot allow\n an empty string as a valid attribute value. The most common work-around is to use a substitute value instead of leaving the field empty.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/DynamoDB/tips/", 
            "text": "There is a \nlocal version of DynamoDB\n provided for developer use.\n\n\nDynamoDB Streams\n provides an ordered stream of changes to a table. Use it to replicate, back up, or drive events off of data\n\n\nDynamoDB can be used \nas a simple locking service\n.\n\n\nDynamoDB indexing can include \nprimary keys\n, which can either be a single-attribute hash key or a composite hash-key range. You can also query non-primary key attributes using \nsecondary indexes\n.\n\n\nData Types:\n DynamoDB supports three \ndata types\n  \nnumber\n, \nstring\n, and \nbinary\n  in both scalar and multi-valued sets. DynamoDB can also support \nJSON\n.", 
            "title": "Tips"
        }, 
        {
            "location": "/EBS/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nEBS\n (Elastic Block Store) provides block level storage. That is, it offers storage volumes that can be attached as filesystems, like traditional network drives.\n\n\nEBS volumes can only be attached to one EC2 instance at a time. In contrast, EFS can be shared but has a much higher price point (\na comparison\n).", 
            "title": "Basics"
        }, 
        {
            "location": "/EBS/gotchas-and-limitations/", 
            "text": "EBS durability is reasonably good for a regular hardware drive (annual failure rate of \nbetween 0.1% - 0.2%\n). On the other hand, that is very poor if you dont have backups! By contrast, S3 durability is extremely high. \nIf you care about your data, back it up to S3 with snapshots.\n\n\nEBS has an \nSLA\n with \n99.95%\n uptime. See notes on high availability below.\n\n\nEBS volumes have a \nvolume type\n indicating the physical storage type. The types called standard (\nst1\n or \nsc1\n) are actually old spinning-platter disks, which deliver only hundreds of IOPS  not what you want unless youre really trying to cut costs. Modern SSD-based \ngp2\n or \nio1\n are typically the options you want.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/EBS/tips/", 
            "text": "RAID:\n Use \nRAID drives\n for \nincreased performance\n.\n\n\nA worthy read is AWS \npost on EBS IO characteristics\n as well as their \nperformance tips\n.\n\n\nOne can \nprovision IOPS\n (that is, pay for a specific level of I/O operations per second) to ensure a particular level of performance for a disk.\n\n\nA single EBS volume allows 10k IOPS max. To get the maximum performance out of an EBS volume, it has to be of a maximum size and attached to an EBS-optimized EC2 instance.\n\n\nA standard block size for an EBS volume is 16kb.", 
            "title": "Tips"
        }, 
        {
            "location": "/EC2/alternatives-and-lock-in/", 
            "text": "Running EC2 is akin to running a set of physical servers, as long as you dont do automatic scaling or tooled cluster setup. If you just run a set of static instances, migrating to another VPS or dedicated server provider should not be too hard.\n\n\nAlternatives to EC2:\n The direct alternatives are Google Cloud, Microsoft Azure, Rackspace, DigitalOcean and other VPS providers, some of which offer similar API for setting up and removing instances. (See the comparisons \nabove\n.)\n\n\nShould you use Amazon Linux?\n AWS encourages use of their own \nAmazon Linux\n, which is evolved from \nRed Hat Enterprise Linux (RHEL)\n and \nCentOS\n. Its used by many, but \nothers are skeptical\n. Whatever you do, think this decision through carefully. Its true Amazon Linux is heavily tested and better supported in the unlikely event you have deeper issues with OS and virtualization on EC2. But in general, many companies do just fine using a standard, non-Amazon Linux distribution, such as Ubuntu or CentOS. Using a standard Linux distribution means you have an exactly replicable environment should you use another hosting provider instead of (or in addition to) AWS. Its also helpful if you wish to test deployments on local developer machines running the same standard Linux distribution (a practice thats getting more common with Docker, too. Amazon now supports an official \nAmazon Linux Docker image\n, aimed at assisting with local development on a comparable environment, though this is new enough that it should be considered experimental).\n\n\nEC2 costs:\n See the \nsection on this\n.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/EC2/basics/", 
            "text": "Homepage\n  \nDocumentation\n  \nFAQ\n  \nPricing\n (see also \nec2instances.info\n)\n\n\nEC2\n (Elastic Compute Cloud) is AWS offering of the most fundamental piece of cloud computing: A \nvirtual private server\n. These instances can run \nmost Linux, BSD, and Windows operating systems\n. Internally, they use \nXen\n virtualization.\n\n\nThe term EC2 is sometimes used to refer to the servers themselves, but technically refers more broadly to a whole collection of supporting services, too, like load balancing (CLBs/ALBs), IP addresses (EIPs), bootable images (AMIs), security groups, and network drives (EBS) (which we discuss individually in this guide).\n\n\nEC2 pricing\n and \ncost management\n is a complicated topic. It can range from free (on the \nAWS free tier\n) to a lot, depending on your usage. Pricing is by instance type, by hour and changes depending on AWS region and whether you are purchasing your instances \nOn-Demand\n, on the \nSpot market\n or pre-purchasing (\nReserved Instances\n).\n\n\nNetwork Performance:\n For some instance types, AWS uses general terms like Low, Medium, and High to refer to network performance. Users have done \nbenchmarking\n to provide expectations for what these terms can mean.", 
            "title": "Basics"
        }, 
        {
            "location": "/EC2/gotchas-and-limitations/", 
            "text": "Never use ssh passwords. Just dont do it; they are too insecure, and consequences of compromise too severe. Use keys instead. \nRead up on this\n and fully disable ssh password access to your ssh server by making sure 'PasswordAuthentication no' is in your /etc/ssh/sshd_config file. If youre careful about managing ssh private keys everywhere they are stored, it is a major improvement on security over password-based authentication.\n\n\nFor all \nnewer instance types\n, when selecting the AMI to use, be sure you select the HVM AMI, or it just wont work.\n\n\nWhen creating an instance and using a new ssh key pair, \nmake sure the ssh key permissions are correct\n.\n\n\nSometimes certain EC2 instances can get scheduled for retirement by AWS due to detected degradation of the underlying hardware, in which case you are given a couple of weeks to migrate to a new instance\n\n\nIf your instance root device is an EBS volume, you can typically stop and then start the instance which moves it to healthy host hardware, giving you control over timing of this event. Note however that you will lose any instance store volume data (\nephemeral drives\n) if your instance type has instance store volumes.\n\n\nThe instance public IP (if it has one) will likely change unless you're using Elastic IPs. This could be a problem if other systems depend on the IP address.\n\n\n\n\n\n\nPeriodically you may find that your server or load balancer is receiving traffic for (presumably) a previous EC2 server that was running at the same IP address that you are handed out now (this may not matter, or it can be fixed by migrating to another new instance).\n\n\nIf the EC2 API itself is a critical dependency of your infrastructure (e.g. for automated server replacement, custom scaling algorithms, etc.) and you are running at a large scale or making many EC2 API calls, make sure that you understand when they might fail (calls to it are \nrate limited\n and the limits are not published and subject to change) and code and test against that possibility.\n\n\nMany newer EC2 instance types are EBS-only. Make sure to factor in EBS performance and costs when planning to use them.\n\n\nInstances come in two types: \nFixed Performance Instances\n (e.g. M3, C3, and R3) and \nBurstable Performance Instances\n (e.g. T2). A T2 instance receives CPU credits continuously, the rate of which depends on the instance size. T2 instances accrue CPU credits when they are idle, and use CPU credits when they are active. However, once an instance runs out of credits, you'll notice a severe degradation in performance. If you need consistently high CPU performance for applications such as video encoding, high volume websites or HPC applications, it is recommended to use Fixed Performance Instances.\n\n\nAn IAM role can be assigned to an EC2 instance \nonly at launch time\n. You cannot assign to a running instance.\n\n\nInstance user-data is \nlimited to 16 KB\n. (This limit applies to the data in raw form, not base64-encoded form.) If more data is needed, it can be downloaded from S3 by a user-data script.\n\n\nVery new accounts may not be able to launch some instance types, such as GPU instances, because of an initially imposed soft limit of zero. This limit can be raised by making a support request. See \nAWS Service Limits\n for the method to make the support request. Note that this limit of zero is \nnot currently documented\n.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/EC2/tips/", 
            "text": "Picking regions:\n When you first set up, consider which \nregions\n you want to use first. Many people in North America just automatically set up in the us-east-1 (N. Virginia) region, which is the default, but its worth considering if this is best up front. For example, you might find it preferable to start in us-west-1 (N. California) or us-west-2 (Oregon) if youre in California and latency matters. Some services \nare not available in all regions\n. Baseline costs also \nvary by region\n, up to 10-30% (generally lowest in us-east-1).\n\n\nInstance types:\n EC2 instances come in many types, corresponding to the capabilities of the virtual machine in CPU architecture and speed, RAM, disk sizes and types (SSD or magnetic), and network bandwidth.\n\n\nSelecting instance types is complex since there are so many types. Additionally, there are different generations, released \nover the years\n.\n\n\nUse the list at \nec2instances.info\n to review costs and features. \nAmazons own list\n of instance types is hard to use, and doesnt list features and price together, which makes it doubly difficult.\n\n\nPrices vary a lot, so use \nec2instances.info\n to determine the set of machines that meet your needs and \nec2price.com\n to find the cheapest type in the region youre working in. Depending on the timing and region, it might be much cheaper to rent an instance with \nmore\n memory or CPU than the bare minimum.\n\n\n\n\n\n\nTurn off\n your instances when they arent in use. For many situations such as testing or staging resources, you may not need your instances on 24/7, and you wont need to pay EC2 hourly costs when they are suspended. Given that costs are calculated based on hourly usage, this is a simple mechanism for cost savings. This can be achieved using \nLambda and CloudWatch\n, an open source solution like \nScalr\n or a SaaS provider like \nGorillaStack\n. (Note: if you turn off instances with an ephemeral root volume, any state will be lost when the instance is turned off. Therefore, for stateful applications it is safer to turn off EBS backed instances).\n\n\nDedicated instances\n and \ndedicated hosts\n are assigned hardware, instead of usual virtual instances. They are more expensive than virtual instances but \ncan be preferable\n for performance, compliance, or licensing reasons.\n\n\n32 bit vs 64 bit:\n A few micro, small, and medium instances are still available to use as 32-bit architecture. Youll be using 64-bit EC2 (amd64) instances nowadays, though smaller instances still support 32 bit (i386). Use 64 bit unless you have legacy constraints or other good reasons to use 32.\n\n\nHVM vs PV:\n There are two kinds of virtualization technology used by EC2, \nhardware virtual machine (HVM) and paravirtual (PV)\n. Historically, PV was the usual type, but \nnow HVM is becoming the standard\n. If you want to use the newest instance types, you must use HVM. See the \ninstance type matrix\n for details.\n\n\nOperating system:\n To use EC2, youll need to pick a base operating system. It can be Windows or Linux, such as Ubuntu or \nAmazon Linux\n. You do this with AMIs, which are covered in more detail in their own section below.\n\n\nLimits:\n You cant create arbitrary numbers of instances. Default limits on numbers of EC2 instances per account vary by instance type, as described in \nthis list\n.\n\n\nUse termination protection:\n For any instances that are important and long-lived (in particular, aren't part of auto-scaling), \nenable termination protection\n. This is an important line of defense against user mistakes, such as accidentally terminating many instances instead of just one due to human error.\n\n\nSSH key management:\n\n\nWhen you start an instance, you need to have at least one \nssh key pair\n set up, to bootstrap, i.e., allow you to ssh in the first time.\n\n\nAside from bootstrapping, you should manage keys yourself on the instances, assigning individual keys to individual users or services as appropriate.\n\n\nAvoid reusing the original boot keys except by administrators when creating new instances.\n\n\nAvoid sharing keys and \nadd individual ssh keys\n for individual users.\n\n\n\n\n\n\n\n\nGPU support:\n You can rent GPU-enabled instances on EC2 for use in machine learning or graphics rendering workloads.\n\n\n\n\nThere are \nthree generations\n of GPU-enabled instances available:\n\n\nThird generation P2 series offers NVIDIA K80 GPUs in 1, 8 and 16 GPU configurations targeting machine learning and scientific workloads.\n\n\nSecond generation G2 series offers NVIDIA K520 GPUs in 1 or 4 GPU configurations targeting graphics and video encoding.\n\n\nFirst generation CG1 instances are still available in some regions in a single configuration with a NVIDIA M2050 GPU.\n\n\n\n\n\n\nAWS offers an \nAMI\n (based on Amazon Linux) with most NVIDIA drivers and ancillary software (CUDA, CUBLAS, CuDNN, TensorFlow) installed to lower the barrier to usage. Note, however, that this leads to lock-in due to Amazon Linux and the fact that you have no direct access to software configuration or versioning.\n\n\nAs with any expensive EC2 instance types, \nSpot instances can offer significant savings\n with GPU workloads when interruptions are tolerable.\n\n\nAll current EC2 instance types can take advantage of IPv6 addressing, so long as they are launched in a subnet with an allocated CIDR range in an IPv6-enabled VPC.", 
            "title": "Tips"
        }, 
        {
            "location": "/ECS/alternatives-and-lock-in/", 
            "text": "Kubernetes\n: Extensive container platform. Available as a hosted solution on Google Cloud (https://cloud.google.com/container-engine/) and AWS (https://tectonic.com/).\n\n\nNomad\n: Orchestrator/Scheduler, tightly integrated in the Hashicorp stack (Consul, Vault, etc).\n\n\n\n\nPlease help expand this incomplete section.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/ECS/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nECS\n (EC2 Container Service) is a relatively new service (launched end of 2014) that manages clusters of services deployed via Docker.\n\n\nSee the \nContainers and AWS\n section for more context on containers.\n\n\nECS is growing in adoption, especially for companies that embrace microservices.\n\n\nDeploying Docker directly in EC2 yourself is another common approach to using Docker on AWS. Using ECS is not required, and ECS does not (yet) seem to be the predominant way many companies are using Docker on AWS.\n\n\nIts also possible to use \nElastic Beanstalk with Docker\n, which is reasonable if youre already using Elastic Beanstalk.\n\n\nUsing Docker may change the way your services are deployed within EC2 or Elastic Beanstalk, but it does not radically change how most other services are used.\n\n\nECR\n (EC2 Container Registry) is Amazons managed Docker registry service. While simpler than running your own registry, it is missing some features that might be desired by some users:\n\n\nDoesnt support cross-region replication of images.\n\n\nIf you want fast fleet-wide pulls of large images, youll need to push your image into a region-local registry.\n\n\n\n\n\n\nDoesnt support custom domains / certificates.\n\n\n\n\n\n\nA containers health is monitored via \nCLB\n or \nALB\n. Those can also be used to address a containerized service. When using an ALB you do not need to handle port contention (i.e. services exposing the same port on the same host) since an ALBs target groups can be associated with ECS-based services directly.\n\n\n\n\nECS supports multiple log drivers (awslogs, splunk, fluentd, syslog, json, ... ). Use \nawslogs\n for CloudWatch (make sure a group is made for the logs first). \nDrivers such as fluentd are not enabled by default\n. You can, install the agent and enable the driver by adding \nECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"fluentd\"]'\n to \n/etc/ecs/ecs.config\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/ECS/tips/", 
            "text": "Log drivers:\n ECS supports multiple log drivers (awslogs, splunk, fluentd, syslog, JSON, ...). Use \nawslogs\n for CloudWatch (make sure a group is made for the logs first). Drivers such as fluentd are not enabled by default. You can install the agent and enable the driver by adding \nECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"fluentd\"]'\n to \n/etc/ecs/ecs.config\n.\n\n\nThis blog from Convox\n (and \ncommentary\n) lists a number of common challenges with ECS as of early 2016.\n\n\nIt is possible to optimize disk clean up on ECS. By default, the unused containers are deleted after 3 hours and the unused images after 30 minutes. These settings can be changed by adding \nECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=10m\n and \nECS_IMAGE_CLEANUP_INTERVAL=10m\n to \n/etc/ecs/ecs.config\n. \nMore information on optimizing ECS disk cleanup\n.", 
            "title": "Tips"
        }, 
        {
            "location": "/EFS/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nEFS\n is Amazons new (general release 2016) network filesystem.\n\n\nIt is designed to be highly available and durable and each EFS file system object is redundantly stored across multiple availability zones.\n\n\nEFS is designed to be used as a shared network drive and it can automatically scale up to petabytes of stored data and thousands of instances attached to it.\n\n\nIts presented as an \nNFSv4.1\n server, so any compatible NFS client can mount it.\n\n\nEFS can offer \nhigher throughput\n (multiple gigabytes per second) and better durability and availability than EBS (see \nthe comparison table\n), but with higher latency.\n\n\nEFS is priced based on the amount of data stored and it costs \nmuch more than EBS\n, about three times as much compared to general purpose gp2 EBS volumes.\n\n\nPerformance\n depends on the amount of data stored on it, which also determines the price:\n\n\nLike EBS, EFS uses a credit based system. Credits are earned at a rate of 50 KiB/s per GiB of storage and consumed in bursts during reading/writing files or metadata. Unlike EBS, operations on metadata (file size, owner, date, etc.) also consume credits. The \nBurstCreditBalance metric\n in CloudWatch should be monitored to make sure the file system doesn't run out of credits.\n\n\nThroughput capacity during bursts is also dependant on size. Under 1 TiB, throughput can go up to 100 MiB/s. Above that, 100 MiB/s is added for each stored TiB. So a file system storing 5 TiB would be able to burst at a rate of 500 MiB/s. Maximum throughput per EC2 instance is 250 MiB/s.\n\n\nEFS has two performance modes that can only be set when a file system is created. One is \"General Purpose\", the other is \"Max I/O\". Max I/O scales higher, but at the cost of higher latency. When in doubt, use General Purpose, which is also the default. If the \nPercentIOLimit metric\n in CloudWatch hovers around 100%, Max I/O is recommended. Changing performance mode means creating a new EFS and migrating data.\n\n\n\n\n\n\nHigh availability is achieved by having \nmount targets in different subnets / availability zones\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/EFS/gotchas-and-limitations/", 
            "text": "A number of NFSv4.1 features are \nnot supported\n and there are some \nlimits\n to the service.\n\n\nAs of 2016-11, EFS does not offer disk level encryption, though it is on the roadmap.\n\n\nSome applications, like SQLite and IPython, \nmight not work properly\n on EFS when accessed from multiple clients. This is because lock upgrades and downgrades are \nnot supported\n. There might be \nworkarounds\n for some issues.\n\n\nAn EFS file system \ncan be mounted on premises\n over Direct Connect.\n\n\nAn EFS file system can NOT be mounted over VPC peering or VPN, even if the VPN is running on top of Direct Connect.\n\n\nUsing an EFS volume on Windows is not supported.\n\n\nWhen a file is uploaded to EFS, it can take hours for EFS to update the details for billing and burst credit purposes.\n\n\nMetadata operations can be costly in terms of burst credit consumption. Recursively traversing a tree containing thousands of files can easily ramp up to tens or even hundreds of megabytes of burst credits being consumed, even if no file is being touched. Commands like \nfind\n or \nchown -R\n can have an adverse impact on performace if run periodically.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/EFS/tips/", 
            "text": "With EFS being based on NFSv4.1, any directory on the EFS can be mounted directly, it doesn't have to be the root directory. One application could mount \nfs-12345678:/prog1\n, another \nfs-12345678:/prog2\n.\n\n\nUser and group level permissions\n can be used to control access to certain directories on the EFS file system.\n\n\n\n\nSharing EFS filesystems:\n One EFS filesystem can be used for multiple applications or services, but it should be considered carefully:\n\n\nPros:\n- Because performance is based on total size of stored files, having everything on one drive will increase performance for everyone. One application consuming credits faster than it can accumulate might be offset by another application that just stores files on EFS and rarely accesses them.\n\n\nCons:\n- Since credits are shared, if one application over-consumes them, it will affect the others.\n- A compromise is made with regards to \nsecurity\n. All clients will have to have network access to the drive. Someone with root access on one client instance can mount any directory on the EFS and they have read-write access to all files on the drive, even if they don't have access to the applications hosted on other clients.", 
            "title": "Tips"
        }, 
        {
            "location": "/EMR/alternatives-and-lock-in/", 
            "text": "Most of EMR is based on open source technology that you can in principle deploy yourself. However, the job workflows and much other tooling is AWS-specific. Migrating from EMR to your own clusters is possible but not always trivial.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/EMR/basics/", 
            "text": "Homepage\n  \nRelease guide\n  \nFAQ\n  \nPricing\n\n\nEMR\n (which used to stand for Elastic Map Reduce, but not anymore, since it now extends beyond map-reduce) is a service that offers managed deployment of \nHadoop\n, \nHBase\n and \nSpark\n. It reduces the management burden of setting up and maintaining these services yourself.", 
            "title": "Basics"
        }, 
        {
            "location": "/EMR/gotchas-and-limitations/", 
            "text": "EMR costs\n can pile up quickly since it involves lots of instances, efficiency can be poor depending on cluster configuration and choice of workload, and accidents like hung jobs are costly. See the \nsection on EC2 cost management\n, especially the tips there about Spot instances and avoiding hourly billing. \nThis blog post\n has additional tips.\n\n\nBeware of double-dipping. With EMR, you pay for the EC2 capacity and the service fees. In addition, EMR syncs task logs to S3, which means you pay for the storage and \nPUT requests\n at \nS3 standard rates\n. While the log files tend to be relatively small, every Hadoop job, depending on the size, generates thousands of log files that can quickly add up to thousands of dollars on the AWS bill. YARNs \nlog aggregation\n is not available on EMR.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/EMR/tips/", 
            "text": "EMR relies on many versions of Hadoop and other supporting software. Be sure to check \nwhich versions are in use\n.\n\n\nOff-the-shelf EMR and Hadoop can have significant overhead when compared with efficient processing on a single machine. If your data is small and performance matters, you may wish to consider alternatives, as \nthis post\n illustrates.\n\n\nPython programmers may want to take a look at Yelps \nmrjob\n.\n\n\nHourly pricing roundoff:\n Since EMR jobs are billed at one-hour granularity, considering changing the number and/or type of instances that your job runs in order to best make use of that time slice (fewer / smaller instances to make more efficient use of an undersubscribed hour, more / larger instances to reduce your jobs runtime).\n\n\nIt takes time to tune performance of EMR jobs, which is why third-party services such as \nQuboles data service\n are gaining popularity as ways to improve performance or reduce costs.", 
            "title": "Tips"
        }, 
        {
            "location": "/Elastic IPs/basics/", 
            "text": "Documentation\n  \nFAQ\n  \nPricing\n\n\nElastic IPs\n are static IP addresses you can rent from AWS to assign to EC2 instances.", 
            "title": "Basics"
        }, 
        {
            "location": "/Elastic IPs/gotchas-and-limitations/", 
            "text": "There is \nofficially no way\n to allocate a contiguous block of IP addresses, something you may desire when giving IPs to external users. Though when allocating at once, you may get lucky and have some be part of the same CIDR block.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Elastic IPs/tips/", 
            "text": "Prefer load balancers to elastic IPs:\n For single-instance deployments, you could just assign elastic IP to an instance, give that IP a DNS name, and consider that your deployment. Most of the time, you should provision a \nload balancer\n instead:\n\n\nIts easy to add and remove instances from load balancers. Its also quicker to add or remove instances from a load balancer than to reassign an elastic IP.\n\n\nIts more convenient to point DNS records to load balancers, instead of pointing them to specific IPs you manage manually. They can also be Route 53 aliases, which are easier to change and manage.\n\n\nBut in some situations, you do need to manage and fix IP addresses of EC2 instances, for example if a customer needs a fixed IP. These situations require elastic IPs.\n\n\n\n\n\n\nElastic IPs are limited to 5 per account. Its possible to \nrequest more\n.\n\n\nIf an Elastic IP is not attached to an active resource there is a small \nhourly fee\n.\n\n\nElastic IPs are \nno extra charge\n as long as youre using them. They have a (small) cost when not in use, which is a mechanism to prevent people from squatting on excessive numbers of IP addresses.", 
            "title": "Tips"
        }, 
        {
            "location": "/Elastic Load Balancer/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nClassic Load Balancers, formerly known as Elastic Load Balancers, are HTTP and TCP load balancers that are managed and scaled for you by Amazon.", 
            "title": "Basics"
        }, 
        {
            "location": "/Elastic Load Balancer/gotchas-and-limitations/", 
            "text": "In general, CLBs are not as smart as some load balancers, and dont have fancy features or fine-grained control a traditional hardware load balancer would offer. For most common cases involving sessionless apps or cookie-based sessions over HTTP, or SSL termination, they work well.\n\n\nBy default, CLBs will refuse to route traffic from a load balancer in one Availability Zone (AZ) to a backend instance in another. This \nwill cause 503s\n if the last instance in an AZ becomes unavailable, even if there are healthy instances in other zones. If youre running fewer than two backend instances per AZ, you almost certainly want to \nenable cross-zone load balancing\n.\n\n\nComplex rules for directing traffic are not supported. For example, you cant direct traffic based on a regular expression in the URL, like \nHAProxy\n offers.\n\n\nApex DNS names:\n Once upon a time, you couldnt assign an CLB to an apex DNS record (i.e. example.com instead of foo.example.com) because it needed to be an A record instead of a CNAME. This is now possible with a Route 53 alias record directly pointing to the load balancer.\n\n\nCLBs use \nHTTP keep-alives\n on the internal side. This can cause an unexpected side effect: Requests from different clients, each in their own TCP connection on the external side, can end up on the same TCP connection on the internal side. Never assume that multiple requests on the same TCP connection are from the same client!\n\n\nTraffic between CLBs and back-end instances in the same subnet \nwill\n have \nNetwork ACL\n rules evaluated (EC2 to EC2 traffic in the same subnet would not have Network ACL rules evaluated). If the default '0.0.0.0/0 ALLOW' rule is removed from the Network ACL applied to the subnet, a rule that allows traffic on both the health check port and any listener port must be added.\n\n\nAs of December 2016, CLBs launched in VPCs do not support IPv6 addressing. CLBs launched in EC2-Classic support both IPv4 and IPv6 \nwith the \"dualstack\" DNS name\n.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Elastic Load Balancer/tips/", 
            "text": "Best practices:\n \nThis article\n is a must-read if you use CLBs heavily, and has a lot more detail.", 
            "title": "Tips"
        }, 
        {
            "location": "/Further Reading/basics/", 
            "text": "This section covers a few unusually useful or must know about resources or lists.\n\n\n\n\nAWS\n\n\nAWS In Plain English\n: A readable overview of all the AWS services\n\n\nAwesome AWS\n: A curated list of AWS tools and software\n\n\nAWS Tips I Wish I'd Known Before I Started\n: A list of tips from \nRich Adams\n\n\nAWS Whitepapers\n: A list of technical AWS whitepapers, covering topics such as architecture, security and economics.\n\n\n\n\n\n\nBooks\n\n\nAmazon Web Services in Action\n\n\nAWS Lambda in Action\n\n\nServerless Architectures on AWS\n\n\nServerless Single Page Apps\n\n\nThe Terraform Book\n\n\nAWS Scripted 2 book series\n\n\nAmazon Web Services For Dummies\n\n\nAWS System Administration\n\n\nPython and AWS Cookbook\n\n\nResilience and Reliability on AWS\n\n\nAWS documentation as Kindle ebooks\n\n\n\n\n\n\nGeneral references\n\n\nAWS Well Architected Framework Guide\n: Amazons own 56 page guide to operational excellence - guidelines and checklists to validate baseline security, reliability, performance (including high availability) and cost optimization practices.\n\n\nAwesome Microservices\n: A curated list of tools and technologies for microservice architectures. Worth browsing to learn about popular open source projects.\n\n\nIs it fast yet?\n: Ilya Grigoriks TLS performance overview\n\n\nHigh Performance Browser Networking\n: A full, modern book on web network performance; a presentation on the HTTP/2 portion is \nhere\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/General Information/aws-product-maturity-and-releases/", 
            "text": "Its important to know the maturity of each AWS product. Here is a mostly complete list of first release date, with links to the \nrelease notes\n. Most recently released services are first. Not all services are available in all regions; see \nthis table\n.\n\n\n\n\n\n\n\n\nService\n\n\nOriginal release\n\n\nAvailability\n\n\nCLI Support\n\n\n\n\n\n\n\n\n\n\nLex\n\n\n2016-11\n\n\nPreview\n\n\n\n\n\n\n\n\nPolly\n\n\n2016-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nRekognition\n\n\n2016-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nDatabase Migration Service\n\n\n2016-03\n\n\nGeneral\n\n\n\n\n\n\n\n\nCertificate Manager\n\n\n2016-01\n\n\nGeneral\n\n\n\n\n\n\n\n\nIoT\n\n\n2015-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nWAF\n\n\n2015-10\n\n\nGeneral\n\n\n\n\n\n\n\n\nData Pipeline\n\n\n2015-10\n\n\nGeneral\n\n\n\n\n\n\n\n\nElasticsearch\n\n\n2015-10\n\n\nGeneral\n\n\n\n\n\n\n\n\nService Catalog\n\n\n2015-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nDevice Farm\n\n\n2015-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nCodePipeline\n\n\n2015-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nCodeCommit\n\n\n2015-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nAPI Gateway\n\n\n2015-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nConfig\n\n\n2015-06\n\n\nGeneral\n\n\n\n\n\n\n\n\nEFS\n\n\n2015-05\n\n\nGeneral\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n2015-04\n\n\nGeneral\n\n\n\n\n\n\n\n\nLambda\n\n\n2014-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nECS\n\n\n2014-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nKMS\n\n\n2014-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nCodeDeploy\n\n\n2014-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nKinesis\n\n\n2013-12\n\n\nGeneral\n\n\n\n\n\n\n\n\nCloudTrail\n\n\n2013-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nAppStream\n\n\n2013-11\n\n\nPreview\n\n\n\n\n\n\n\n\nCloudHSM\n\n\n2013-03\n\n\nGeneral\n\n\n\n\n\n\n\n\nSilk\n\n\n2013-03\n\n\nObsolete?\n\n\n\n\n\n\n\n\nOpsWorks\n\n\n2013-02\n\n\nGeneral\n\n\n\n\n\n\n\n\nRedshift\n\n\n2013-02\n\n\nGeneral\n\n\n\n\n\n\n\n\nElastic Transcoder\n\n\n2013-01\n\n\nGeneral\n\n\n\n\n\n\n\n\nGlacier\n\n\n2012-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nCloudSearch\n\n\n2012-04\n\n\nGeneral\n\n\n\n\n\n\n\n\nSWF\n\n\n2012-02\n\n\nGeneral\n\n\n\n\n\n\n\n\nStorage Gateway\n\n\n2012-01\n\n\nGeneral\n\n\n\n\n\n\n\n\nDynamoDB\n\n\n2012-01\n\n\nGeneral\n\n\n\n\n\n\n\n\nDirectConnect\n\n\n2011-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nElastiCache\n\n\n2011-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nCloudFormation\n\n\n2011-04\n\n\nGeneral\n\n\n\n\n\n\n\n\nSES\n\n\n2011-01\n\n\nGeneral\n\n\n\n\n\n\n\n\nElastic Beanstalk\n\n\n2010-12\n\n\nGeneral\n\n\n\n\n\n\n\n\nRoute 53\n\n\n2010-10\n\n\nGeneral\n\n\n\n\n\n\n\n\nIAM\n\n\n2010-09\n\n\nGeneral\n\n\n\n\n\n\n\n\nSNS\n\n\n2010-04\n\n\nGeneral\n\n\n\n\n\n\n\n\nEMR\n\n\n2010-04\n\n\nGeneral\n\n\n\n\n\n\n\n\nRDS\n\n\n2009-12\n\n\nGeneral\n\n\n\n\n\n\n\n\nVPC\n\n\n2009-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nSnowball\n\n\n2009-05\n\n\nGeneral\n\n\n\n\n\n\n\n\nCloudWatch\n\n\n2009-05\n\n\nGeneral\n\n\n\n\n\n\n\n\nCloudFront\n\n\n2008-11\n\n\nGeneral\n\n\n\n\n\n\n\n\nFulfillment Web Service\n\n\n2008-03\n\n\nObsolete?\n\n\n\n\n\n\n\n\nSimpleDB\n\n\n2007-12\n\n\nNearly obsolete\n\n\n\n\n\n\n\n\nDevPay\n\n\n2007-12\n\n\nGeneral\n\n\n\n\n\n\n\n\nFlexible Payments Service\n\n\n2007-08\n\n\nRetired\n\n\n\n\n\n\n\n\nEC2\n\n\n2006-08\n\n\nGeneral\n\n\n\n\n\n\n\n\nSQS\n\n\n2006-07\n\n\nGeneral\n\n\n\n\n\n\n\n\nS3\n\n\n2006-03\n\n\nGeneral\n\n\n\n\n\n\n\n\nAlexa Top Sites\n\n\n2006-01\n\n\nGeneral HTTP-only\n\n\n\n\n\n\n\n\nAlexa Web Information Service\n\n\n2005-10\n\n\nGeneral HTTP-only", 
            "title": "Aws product maturity and releases"
        }, 
        {
            "location": "/General Information/common-concepts/", 
            "text": "The AWS \nGeneral Reference\n covers a bunch of common concepts that are relevant for multiple services.\n\n\nAWS allows deployments in \nregions\n, which are isolated geographic locations that help you reduce latency or offer additional redundancy (though typically availability zones are the first tool of choice for \nhigh availability\n).\n\n\nEach service has API \nendpoints\n for each region. Endpoints differ from service to service and not all services are available in each region, as listed in \nthese tables\n.\n\n\nAmazon Resource Names (ARNs)\n are specially formatted identifiers for identifying resources. They start with 'arn:' and are used in many services, and in particular for IAM policies.", 
            "title": "Common concepts"
        }, 
        {
            "location": "/General Information/compliance/", 
            "text": "Many applications have strict requirements around reliability, security, or data privacy. The \nAWS Compliance\n page has details about AWSs certifications, which include \nPCI DSS Level 1\n, \nSOC 3\n, and \nISO 9001\n.\n\n\nSecurity in the cloud is a complex topic, based on a \nshared responsibility model\n, where some elements of compliance are provided by AWS, and some are provided by your company.\n\n\nSeveral third-party vendors offer assistance with compliance, security, and auditing on AWS. If you have substantial needs in these areas, assistance is a good idea.\n\n\nFrom inside \nChina\n, AWS services outside China \nare generally accessible\n, though there are at times breakages in service. There are also AWS services \ninside China\n.", 
            "title": "Compliance"
        }, 
        {
            "location": "/General Information/getting-help-and-support/", 
            "text": "Forums:\n For many problems, its worth searching or asking for help in the \ndiscussion forums\n to see if its a known issue.\n\n\nPremium support:\n AWS offers several levels of \npremium support\n.\n\n\nThe first tier, called \"Developer support\" lets you file support tickets with 12 to 24 hour turnaround time, it starts at $29 but once your monthly spend reaches around $1000 it changes to a 3% surcharge on your bill.\n\n\nThe higher-level support services are quite expensive  and increase your bill by up to 10%. Many large and effective companies never pay for this level of support. They are usually more helpful for midsize or larger companies needing rapid turnaround on deeper or more perplexing problems.\n\n\nKeep in mind, a flexible architecture can reduce need for support. You shouldnt be relying on AWS to solve your problems often. For example, if you can easily re-provision a new server, it may not be urgent to solve a rare kernel-level issue unique to one EC2 instance. If your EBS volumes have recent snapshots, you may be able to restore a volume before support can rectify the issue with the old volume. If your services have an issue in one availability zone, you should in any case be able to rely on a redundant zone or migrate services to another zone.\n\n\nLarger customers also get access to AWS Enterprise support, with dedicated technical account managers (TAMs) and shorter response time SLAs.\n\n\nThere is definitely some controversy about how useful the paid support is. The support staff dont always seem to have the information and authority to solve the problems that are brought to their attention. Often your ability to have a problem solved may depend on your relationship with your account rep.\n\n\n\n\n\n\nAccount manager:\n If you are at significant levels of spend (thousands of US dollars plus per month), you may be assigned (or may wish to ask for) a dedicated account manager.\n\n\nThese are a great resource, even if youre not paying for premium support. Build a good relationship with them and make use of them, for questions, problems, and guidance.\n\n\nAssign a single point of contact on your companys side, to avoid confusing or overwhelming them.\n\n\n\n\n\n\nContact:\n The main web contact point for AWS is \nhere\n. Many technical requests can be made via these channels.\n\n\nConsulting and managed services:\n For more hands-on assistance, AWS has established relationships with many \nconsulting partners\n and \nmanaged service partners\n. The big consultants wont be cheap, but depending on your needs, may save you costs long term by helping you set up your architecture more effectively, or offering specific expertise, e.g. security. Managed service providers provide longer-term full-service management of cloud resources.\n\n\nAWS Professional Services:\n AWS provides \nconsulting services\n alone or in combination with partners.", 
            "title": "Getting help and support"
        }, 
        {
            "location": "/General Information/restrictions-and-other-notes/", 
            "text": "Lots of resources in Amazon have \nlimits\n on them. This is actually helpful, so you dont incur large costs accidentally. You have to request that quotas be increased by opening support tickets. Some limits are easy to raise, and some are not. (Some of these are noted in sections below.)\n\n\nObtaining Current Limits and Usage:\n Limit information for a service may be available from the service API, Trusted Advisor, both or neither (in which case you'll need to contact Support). \nThis page\n from the awslimitchecker tool's documentation provides a nice summary of available retrieval options for each limit. The \ntool\n itself is also valuable for automating limit checks.\n\n\n\n\n\n\nAWS terms of service\n are extensive. Much is expected boilerplate, but it does contain important notes and restrictions on each service. In particular, there are restrictions against using many AWS services in \nsafety-critical systems\n. (Those appreciative of legal humor may wish to review clause 57.10.)", 
            "title": "Restrictions and other notes"
        }, 
        {
            "location": "/General Information/service-matrix/", 
            "text": "Many services within AWS can at least be compared with Google Cloud offerings or with internal Google services. And often times you could assemble the same thing yourself with open source software. This table is an effort at listing these rough correspondences. (Remember that this table is imperfect as in almost every case there are subtle differences of features!)\n\n\n\n\n\n\n\n\nService\n\n\nAWS\n\n\nGoogle Cloud\n\n\nGoogle Internal\n\n\nMicrosoft Azure\n\n\nOther providers\n\n\nOpen source build your own\n\n\n\n\n\n\n\n\n\n\nVirtual server\n\n\nEC2\n\n\nCompute Engine (GCE)\n\n\n\n\nVirtual Machine\n\n\nDigitalOcean\n\n\nOpenStack\n\n\n\n\n\n\nPaaS\n\n\nElastic Beanstalk\n\n\nApp Engine\n\n\nApp Engine\n\n\nWeb Apps\n\n\nHeroku, AppFog, OpenShift\n\n\nMeteor, AppScale, Cloud Foundry, Convox\n\n\n\n\n\n\nServerless, microservices\n\n\nLambda, API Gateway\n\n\nFunctions\n\n\n\n\nFunction Apps\n\n\nPubNub Blocks, Auth0 Webtask\n\n\nKong, Tyk\n\n\n\n\n\n\nContainer, cluster manager\n\n\nECS\n\n\nContainer Engine, Kubernetes\n\n\nBorg or Omega\n\n\nContainer Service\n\n\n\n\nKubernetes, Mesos, Aurora\n\n\n\n\n\n\nFile storage\n\n\nS3\n\n\nCloud Storage\n\n\nGFS\n\n\nStorage Account\n\n\n\n\nSwift, HDFS\n\n\n\n\n\n\nBlock storage\n\n\nEBS\n\n\nPersistent Disk\n\n\n\n\nStorage Account\n\n\nDigitalOcean Volumes\n\n\nNFS\n\n\n\n\n\n\nSQL datastore\n\n\nRDS\n\n\nCloud SQL\n\n\n\n\nSQL Database\n\n\n\n\nMySQL, PostgreSQL\n\n\n\n\n\n\nSharded RDBMS\n\n\n\n\n\n\nF1, Spanner\n\n\n\n\n\n\nCrate.io, CockroachDB\n\n\n\n\n\n\nBigtable\n\n\n\n\nCloud Bigtable\n\n\nBigtable\n\n\n\n\n\n\nHBase\n\n\n\n\n\n\nKey-value store, column store\n\n\nDynamoDB\n\n\nCloud Datastore\n\n\nMegastore\n\n\nTables, DocumentDB\n\n\n\n\nCassandra, CouchDB, RethinkDB, Redis\n\n\n\n\n\n\nMemory cache\n\n\nElastiCache\n\n\nApp Engine Memcache\n\n\n\n\nRedis Cache\n\n\n\n\nMemcached, Redis\n\n\n\n\n\n\nSearch\n\n\nCloudSearch, Elasticsearch (managed)\n\n\n\n\n\n\nSearch\n\n\nAlgolia, QBox\n\n\nElasticsearch, Solr\n\n\n\n\n\n\nData warehouse\n\n\nRedshift\n\n\nBigQuery\n\n\nDremel\n\n\nSQL Data Warehouse\n\n\nOracle, IBM, SAP, HP, many others\n\n\nGreenplum\n\n\n\n\n\n\nBusiness intelligence\n\n\nQuickSight\n\n\nData Studio 360\n\n\n\n\nPower BI\n\n\nTableau\n\n\n\n\n\n\n\n\nLock manager\n\n\nDynamoDB (weak)\n\n\n\n\nChubby\n\n\nLease blobs in Storage Account\n\n\n\n\nZooKeeper, Etcd, Consul\n\n\n\n\n\n\nMessage broker\n\n\nSQS, SNS, IoT\n\n\nPub/Sub\n\n\nPubSub2\n\n\nService Bus\n\n\n\n\nRabbitMQ, Kafka, 0MQ\n\n\n\n\n\n\nStreaming, distributed log\n\n\nKinesis\n\n\nDataflow\n\n\nPubSub2\n\n\nEvent Hubs\n\n\n\n\nKafka Streams, Apex, Flink, Spark Streaming, Storm\n\n\n\n\n\n\nMapReduce\n\n\nEMR\n\n\nDataproc\n\n\nMapReduce\n\n\nHDInsight, DataLake Analytics\n\n\nQubole\n\n\nHadoop\n\n\n\n\n\n\nMonitoring\n\n\nCloudWatch\n\n\nMonitoring\n\n\nBorgmon\n\n\nMonitor\n\n\n\n\nPrometheus(?)\n\n\n\n\n\n\nMetric management\n\n\n\n\n\n\nBorgmon, TSDB\n\n\nApplication Insights\n\n\n\n\nGraphite, InfluxDB, OpenTSDB, Grafana, Riemann, Prometheus\n\n\n\n\n\n\nCDN\n\n\nCloudFront\n\n\nCloud CDN\n\n\n\n\nCDN\n\n\n\n\nApache Traffic Server\n\n\n\n\n\n\nLoad balancer\n\n\nCLB/ALB\n\n\nLoad Balancing\n\n\nGFE\n\n\nLoad Balancer, Application Gateway\n\n\n\n\nnginx, HAProxy, Apache Traffic Server\n\n\n\n\n\n\nDNS\n\n\nRoute53\n\n\nDNS\n\n\n\n\nDNS\n\n\n\n\nbind\n\n\n\n\n\n\nEmail\n\n\nSES\n\n\n\n\n\n\n\n\nSendgrid, Mandrill, Postmark\n\n\n\n\n\n\n\n\nGit hosting\n\n\nCodeCommit\n\n\nCloud Source Repositories\n\n\n\n\nVisual Studio Team Services\n\n\nGitHub, BitBucket\n\n\nGitLab\n\n\n\n\n\n\nUser authentication\n\n\nCognito\n\n\nFirebase Authentication\n\n\n\n\nAzure Active Directory\n\n\n\n\noauth.io\n\n\n\n\n\n\nMobile app analytics\n\n\nMobile Analytics\n\n\nFirebase Analytics\n\n\n\n\nHockeyApp\n\n\nMixpanel\n\n\n\n\n\n\n\n\nMobile app testing\n\n\nDevice Farm\n\n\nFirebase Test Lab\n\n\n\n\nXamarin Test Cloud\n\n\nBrowserStack, Sauce Labs, Testdroid\n\n\n\n\n\n\n\n\nManaging SSL/TLS certificates\n\n\nCertificate Manager\n\n\n\n\n\n\n\n\nLet's Encrypt, Comodo, Symantec, GlobalSign\n\n\n\n\n\n\n\n\nAutomatic speech recognition and natural language understanding\n\n\nLex\n\n\nCloud Speech API, Natural Language API\n\n\n\n\nCognitive services\n\n\nAYLIEN Text Analysis API, Ambiverse Natural Language Understanding API\n\n\nStanford's Core NLP Suite, Apache OpenNLP, Apache UIMA, spaCy\n\n\n\n\n\n\nText-to-speech engine in the cloud\n\n\nPolly\n\n\n\n\n\n\n\n\nNuance, Vocalware, IBM Watson\n\n\nMimic, eSpeak, MaryTTS\n\n\n\n\n\n\nImage recognition\n\n\nRekognition\n\n\nVision API\n\n\n\n\nCognitive services\n\n\nIBM Watson, Clarifai\n\n\nTensorFlow, OpenCV\n\n\n\n\n\n\n\n\nPlease help fill this table in.\n\n\nSelected resources with more detail on this chart:\n\n\n\n\nGoogle internal: \nMapReduce\n, \nBigtable\n, \nSpanner\n, \nF1 vs Spanner\n, \nBigtable vs Megastore", 
            "title": "Service matrix"
        }, 
        {
            "location": "/General Information/tools-and-services-market-landscape/", 
            "text": "There are now enough cloud and big data enterprise companies and products that few can keep up with the market landscape. (See the \nBig Data Evolving Landscape  2016\n for one attempt at this.)\n\n\nWeve assembled a landscape of a few of the services. This is far from complete, but tries to emphasize services that are popular with AWS practitioners  services that specifically help with AWS, or a complementary, or tools almost anyone using AWS must learn.\n\n\n\n\nSuggestions to improve this figure? Please \nfile an issue\n.", 
            "title": "Tools and services market landscape"
        }, 
        {
            "location": "/General Information/when-to-use-aws/", 
            "text": "AWS\n is the dominant public cloud computing provider.\n\n\nIn general, \ncloud computing\n can refer to one of three types of cloud: public, private, and hybrid. AWS is a public cloud provider, since anyone can use it. Private clouds are within a single (usually large) organization. Many companies use a hybrid of private and public clouds.\n\n\nThe core features of AWS are \ninfrastructure-as-a-service\n (IaaS)  that is, virtual machines and supporting infrastructure. Other cloud service models include \nplatform-as-a-service\n (PaaS), which typically are more fully managed services that deploy customers applications, or \nsoftware-as-a-service\n (SaaS), which are cloud-based applications. AWS does offer a few products that fit into these other models, too.\n\n\nIn business terms, with infrastructure-as-a-service you have a variable cost model  it is \nOpEx, not CapEx\n (though some \npre-purchased contracts\n are still CapEx).\n\n\n\n\n\n\nAWSs annual revenue was \n$7.88 billion\n as of 2015 according to their SEC 10-K filing, or roughly \n7%\n of Amazon.coms total 2015 revenue.\n\n\nMain reasons to use AWS:\n\n\nIf your company is building systems or products that may need to scale\n\n\nand you have technical know-how\n\n\nand you want the most flexible tools\n\n\nand youre not significantly tied into different infrastructure already\n\n\nand you dont have internal, regulatory, or compliance reasons you cant use a public cloud-based solution\n\n\nand youre not on a Microsoft-first tech stack\n\n\nand you dont have a specific reason to use Google Cloud\n\n\nand you can afford, manage, or negotiate its somewhat higher costs\n\n\n... then AWS is likely a good option for your company.\n\n\n\n\n\n\nEach of those reasons above might point to situations where other services are preferable. In practice, many, if not most, tech startups as well as a number of modern large companies can or already do benefit from using AWS. Many large enterprises are partly migrating internal infrastructure to Azure, Google Cloud, and AWS.\n\n\nCosts:\n Billing and cost management are such big topics that we have \nan entire section on this\n.\n\n\nEC2 vs. other services:\n Most users of AWS are most familiar with \nEC2\n, AWS flagship virtual server product, and possibly a few others like S3 and CLBs. But AWS products now extend far beyond basic IaaS, and often companies do not properly understand or appreciate all the many AWS services and how they can be applied, due to the \nsharply growing\n number of services, their novelty and complexity, branding confusion, and fear of lock-in to proprietary AWS technology. Although a bit daunting, its important for technical decision-makers in companies to understand the breadth of the AWS services and make informed decisions. (We hope this guide will help.)\n\n\nAWS vs. other cloud providers:\n While AWS is the dominant IaaS provider (31% market share in \nthis 2016 estimate\n), there is significant competition and alternatives that are better suited to some companies. \nThis Gartner report\n has a good overview of the major cloud players :\n\n\nGoogle Cloud\n. It arrived later to market than AWS, but has vast resources and is now used widely by many companies, including a few large ones. It is gaining market share. Not all AWS services have similar or analogous services in Google Cloud. And vice versa: In particular Google offers some more advanced machine learning-based services like the \nVision\n, \nSpeech\n, and \nNatural Language\n APIs. Its not common to switch once youre up and running, but it does happen: \nSpotify migrated\n from AWS to Google Cloud. There is more discussion \non Quora\n about relative benefits.\n\n\nMicrosoft Azure\n is the de facto choice for companies and teams that are focused on a Microsoft stack, and it has now placed significant emphasis on Linux as well\n\n\nIn \nChina\n, AWS footprint is relatively small. The market is dominated by Alibabas \nAliyun\n.\n\n\nCompanies at (very) large scale may want to reduce costs by managing their own infrastructure. For example, \nDropbox migrated\n to their own infrastructure.\n\n\nOther cloud providers such as \nDigital Ocean\n offer similar services, sometimes with greater ease of use, more personalized support, or lower cost. However, none of these match the breadth of products, mind-share, and market domination AWS now enjoys.\n\n\nTraditional managed hosting providers such as \nRackspace\n offer cloud solutions as well.\n\n\n\n\n\n\nAWS vs. PaaS:\n If your goal is just to put up a single service that does something relatively simple, and youre trying to minimize time managing operations engineering, consider a \nplatform-as-a-service\n such as \nHeroku\n. The AWS approach to PaaS, Elastic Beanstalk, is arguably more complex, especially for simple use cases.\n\n\nAWS vs. web hosting:\n If your main goal is to host a website or blog, and you dont expect to be building an app or more complex service, you may wish consider one of the myriad \nweb hosting services\n.\n\n\nAWS vs. managed hosting:\n Traditionally, many companies pay \nmanaged hosting\n providers to maintain physical servers for them, then build and deploy their software on top of the rented hardware. This makes sense for businesses who want direct control over hardware, due to legacy, performance, or special compliance constraints, but is usually considered old fashioned or unnecessary by many developer-centric startups and younger tech companies.\n\n\nComplexity:\n AWS will let you build and scale systems to the size of the largest companies, but the complexity of the services when used at scale requires significant depth of knowledge and experience. Even very simple use cases often require more knowledge to do right in AWS than in a simpler environment like Heroku or Digital Ocean. (This guide may help!)\n\n\nGeographic locations:\n AWS has data centers in \nover a dozen geographic locations\n, known as \nregions\n, in Europe, East Asia, North and South America, and now Australia and India. It also has many more \nedge locations\n globally for reduced latency of services like CloudFront.\n\n\nSee the \ncurrent list\n of regions and edge locations, including upcoming ones.\n\n\nIf your infrastructure needs to be in close physical proximity to another service for latency or throughput reasons (for example, latency to an ad exchange), viability of AWS may depend on the location.\n\n\n\n\n\n\nLock-in:\n As you use AWS, its important to be aware when you are depending on AWS services that do not have equivalents elsewhere.\n\n\nLock-in may be completely fine for your company, or a significant risk. Its important from a business perspective to make this choice explicitly, and consider the cost, operational, business continuity, and competitive risks of being tied to AWS. AWS is such a dominant and reliable vendor, many companies are comfortable with using AWS to its full extent. Others can tell stories about the \ndangers of cloud jail when costs spiral\n.\n\n\nGenerally, the more AWS services you use, the more lock-in you have to AWS  that is, the more engineering resources (time and money) it will take to change to other providers in the future.\n\n\nBasic services like virtual servers and standard databases are usually easy to migrate to other providers or on premises. Others like load balancers and IAM are specific to AWS but have close equivalents from other providers. The key thing to consider is whether engineers are architecting systems around specific AWS services that are not open source or relatively interchangeable. For example, Lambda, API Gateway, Kinesis, Redshift, and DynamoDB do not have substantially equivalent open source or commercial service equivalents, while EC2, RDS (MySQL or Postgres), EMR, and ElastiCache more or less do. (See more \nbelow\n, where these are noted with .)\n\n\n\n\n\n\nCombining AWS and other cloud providers:\n Many customers combine AWS with other non-AWS services. For example, legacy systems or secure data might be in a managed hosting provider, while other systems are AWS. Or a company might only use S3 with another provider doing everything else. However small startups or projects starting fresh will typically stick to AWS or Google Cloud only.\n\n\nHybrid cloud:\n In larger enterprises, it is common to have \nhybrid deployments\n encompassing private cloud or on-premises servers and AWS  or other enterprise cloud providers like \nIBM\n/\nBluemix\n, \nMicrosoft\n/\nAzure\n, \nNetApp\n, or \nEMC\n.\n\n\nMajor customers:\n Who uses AWS and Google Cloud?\n\n\nAWSs \nlist of customers\n includes large numbers of mainstream online properties and major brands, such as Netflix, Pinterest, Spotify (moving to Google Cloud), Airbnb, Expedia, Yelp, Zynga, Comcast, Nokia, and Bristol-Myers Squibb.\n\n\nAzures \nlist of customers\n includes companies such as NBC Universal, 3M and Honeywell Inc.\n\n\n\n\n\n\nGoogle Clouds \nlist of customers\n is large as well, and includes a few mainstream sites, such as \nSnapchat\n, Best Buy, Dominos, and Sony Music.", 
            "title": "When to use aws"
        }, 
        {
            "location": "/General Information/which-services-to-use/", 
            "text": "AWS offers a \nlot\n of different services  \nabout fifty\n at last count.\n\n\nMost customers use a few services heavily, a few services lightly, and the rest not at all. What services youll use depends on your use cases. Choices differ substantially from company to company.\n\n\nImmature and unpopular services:\n Just because AWS has a service that sounds promising, it doesnt mean you should use it. Some services are very narrow in use case, not mature, are overly opinionated, or have limitations, so building your own solution may be better. We try to give a sense for this by breaking products into categories.\n\n\nMust-know infrastructure:\n Most typical small to medium-size users will focus on the following services first. If you manage use of AWS systems, you likely need to know at least a little about all of these. (Even if you dont use them, you should learn enough to make that choice intelligently.)\n\n\nIAM\n: User accounts and identities (you need to think about accounts early on!)\n\n\nEC2\n: Virtual servers and associated components, including:\n\n\nAMIs\n: Machine Images\n\n\nLoad Balancers\n: CLBs and ALBs\n\n\nAutoscaling\n: Capacity scaling (adding and removing servers based on load)\n\n\nEBS\n: Network-attached disks\n\n\nElastic IPs\n: Assigned IP addresses\n\n\n\n\n\n\nS3\n: Storage of files\n\n\nRoute 53\n: DNS and domain registration\n\n\nVPC\n: Virtual networking, network security, and co-location; you automatically use\n\n\nCloudFront\n: CDN for hosting content\n\n\nCloudWatch\n: Alerts, paging, monitoring\n\n\n\n\n\n\nManaged services:\n Existing software solutions you could run on your own, but with managed deployment:\n\n\nRDS\n: Managed relational databases (managed MySQL, Postgres, and Amazons own Aurora database)\n\n\nEMR\n: Managed Hadoop\n\n\nElasticsearch\n: Managed Elasticsearch\n\n\nElastiCache\n: Managed Redis and Memcached\n\n\n\n\n\n\nOptional but important infrastructure:\n These are key and useful infrastructure components that are less widely known and used. You may have legitimate reasons to prefer alternatives, so evaluate with care to be sure they fit your needs:\n\n\nLambda\n: Running small, fully managed tasks serverless\n\n\nCloudTrail\n: AWS API logging and audit (often neglected but important)\n\n\nCloudFormation\n: Templatized configuration of collections of AWS resources\n\n\nElastic Beanstalk\n: Fully managed (PaaS) deployment of packaged Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker applications\n\n\nEFS\n: Network filesystem compatible with NFSv4.1\n\n\nECS\n: Docker container/cluster management (note Docker can also be used directly, without ECS)\n\n\nECR\n: Hosted private Docker registry\n\n\nConfig\n: AWS configuration inventory, history, change notifications\n\n\n\n\n\n\nSpecial-purpose infrastructure:\n These services are focused on specific use cases and should be evaluated if they apply to your situation. Many also are proprietary architectures, so tend to tie you to AWS.\n\n\nDynamoDB\n: Low-latency NoSQL key-value store\n\n\nGlacier\n: Slow and cheap alternative to S3\n\n\nKinesis\n: Streaming (distributed log) service\n\n\nSQS\n: Message queueing service\n\n\nRedshift\n: Data warehouse\n\n\nQuickSight\n: Business intelligence service\n\n\nSES\n: Send and receive e-mail for marketing or transactions\n\n\nAPI Gateway\n: Proxy, manage, and secure API calls\n\n\nIoT\n: Manage bidirectional communication over HTTP, WebSockets, and MQTT between AWS and clients (often but not necessarily things like appliances or sensors)\n\n\nWAF\n: Web firewall for CloudFront to deflect attacks\n\n\nKMS\n: Store and manage encryption keys securely\n\n\nInspector\n: Security audit\n\n\nTrusted Advisor\n: Automated tips on reducing cost or making improvements\n\n\nCertificate Manager\n: Manage SSL/TLS certificates for AWS services\n\n\n\n\n\n\nCompound services:\n These are similarly specific, but are full-blown services that tackle complex problems and may tie you in. Usefulness depends on your requirements. If you have large or significant need, you may have these already managed by in-house systems and engineering teams.\n\n\nMachine Learning\n: Machine learning model training and classification\n\n\nLex\n: Automatic speech recognition (ASR) and natural language understanding (NLU)\n\n\nPolly\n: Text-to-speech engine in the cloud\n\n\nRekognition\n: Service for image recognition\n\n\nData Pipeline\n: Managed ETL service\n\n\nSWF\n: Managed state tracker for distributed polyglot job workflow\n\n\nLumberyard\n: 3D game engine\n\n\n\n\n\n\nMobile/app development:\n\n\nSNS\n: Manage app push notifications and other end-user notifications\n\n\nCognito\n: User authentication via Facebook, Twitter, etc.\n\n\nDevice Farm\n: Cloud-based device testing\n\n\nMobile Analytics\n: Analytics solution for app usage\n\n\nMobile Hub\n: Comprehensive, managed mobile app framework\n\n\n\n\n\n\nEnterprise services:\n These are relevant if you have significant corporate cloud-based or hybrid needs. Many smaller companies and startups use other solutions, like Google Apps or Box. Larger companies may also have their own non-AWS IT solutions.\n\n\nAppStream\n: Windows apps in the cloud, with access from many devices\n\n\nWorkspaces\n: Windows desktop in the cloud, with access from many devices\n\n\nWorkDocs\n (formerly Zocalo): Enterprise document sharing\n\n\nWorkMail\n: Enterprise managed e-mail and calendaring service\n\n\nDirectory Service\n: Microsoft Active Directory in the cloud\n\n\nDirect Connect\n: Dedicated network connection between office or data center and AWS\n\n\nStorage Gateway\n: Bridge between on-premises IT and cloud storage\n\n\nService Catalog\n: IT service approval and compliance\n\n\n\n\n\n\nProbably-don't-need-to-know services:\n Bottom line, our informal polling indicates these services are just not broadly used  and often for good reasons:\n\n\nSnowball\n: If you want to ship petabytes of data into or out of Amazon using a physical appliance, read on.\n\n\nSnowmobile\n: Appliances are great, but if you've got exabyte scale data to get into Amazon, nothing beats a tractor trailer full of drives.\n\n\nCodeCommit\n: Git service. Youre probably already using GitHub or your own solution (\nStackshare\n has informal stats).\n\n\nCodePipeline\n: Continuous integration. You likely have another solution already.\n\n\nCodeDeploy\n: Deployment of code to EC2 servers. Again, you likely have another solution.\n\n\nOpsWorks\n: Management of your deployments using Chef. While Chef is popular, it seems few people use OpsWorks, since it involves going in on a whole different code deployment framework.\n\n\n\n\n\n\nAWS in Plain English\n offers more friendly explanation of what all the other different services are.", 
            "title": "Which services to use"
        }, 
        {
            "location": "/Glacier/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nGlacier\n is a lower-cost alternative to S3 when data is infrequently accessed, such as for archival purposes.\n\n\nIts only useful for data that is rarely accessed. It generally takes \n3-5 hours\n to fulfill a retrieval request.\n\n\nAWS \nhas not officially revealed\n the storage media used by Glacier; it may be low-spin hard drives or even tapes.", 
            "title": "Basics"
        }, 
        {
            "location": "/Glacier/gotchas-and-limitations/", 
            "text": "Getting files off Glacier is glacially slow (typically 3-5 hours or more).\n\n\nDue to a fixed overhead per file (you pay per PUT or GET operation), uploading and downloading many small files on/to Glacier might be very expensive. There is also a 32k storage overhead per file. Hence its a good idea is to archive files before upload.\n\n\nGlaciers pricing policy is reportedly pretty complicated: Glacier data retrievals are priced based on the peak hourly retrieval capacity used within a calendar month. Some more info can be found \nhere\n and \nhere\n.\n\n\nBe aware of the per-object costs of archiving S3 data to Glacier. \nIt costs $0.05 per 1,000 requests\n. If you have large numbers of S3 objects of relatively small size, \nit will take time to reach a break-even point\n (initial archiving cost versus lower storage pricing).", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Glacier/tips/", 
            "text": "You can physically \nship\n your data to Amazon to put on Glacier on a USB or eSATA HDD.", 
            "title": "Tips"
        }, 
        {
            "location": "/High Availability/gotchas-and-limitations/", 
            "text": "AZ naming\n differs from one customer account to the next. Your us-west-1a is not the same as another customers us-west-1a  the letters are assigned to physical AZs randomly per account. This can also be a gotcha if you have multiple AWS accounts.\n\n\nCross-AZ traffic\n is not free. At large scale, the costs add up to a significant amount of money. If possible, optimize your traffic to stay within the same AZ as much as possible.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/High Availability/tips/", 
            "text": "AWS offers two levels of redundancy, \nregions and availability zones (AZs)\n.\n\n\nWhen used correctly, regions and zones do allow for high availability. You may want to use non-AWS providers for larger business risk mitigation (i.e. not tying your company to one vendor), but reliability of AWS across regions is very high.\n\n\nMultiple regions:\n Using multiple regions is complex, since its essentially like managing completely separate infrastructures. It is necessary for business-critical services with the highest levels of redundancy. However, for many applications (like your average consumer startup), deploying extensive redundancy across regions may be overkill.\n\n\nThe \nHigh Scalability Blog\n has a good guide to help you understand when you need to scale an application to multiple regions.\n\n\nMultiple AZs:\n Using AZs wisely is the primary tool for high availability!\n\n\nA typical single-region high availability architecture would be to deploy in two or more availability zones, with load balancing in front, as in \nthis AWS diagram\n.\n\n\nThe bulk of outages in AWS services affect one zone only. There have been rare outages affecting multiple zones simultaneously (for example, the \ngreat EBS failure of 2011\n) but in general most customers outages are due to using only a single AZ for some infrastructure.\n\n\nConsequently, design your architecture to minimize the impact of AZ outages, especially single-zone outages.\n\n\nDeploy key infrastructure across at least two or three AZs. Replicating a single resource across more than three zones often wont make sense if you have other backup mechanisms in place, like S3 snapshots.\n\n\nA second or third AZ should significantly improve availability, but additional reliability of 4 or more AZs may not justify the costs or complexity (unless you have other reasons like capacity or Spot market prices).\n\n\nWatch out for \ncross-AZ traffic costs\n. This can be an unpleasant surprise in architectures with large volume of traffic crossing AZ boundaries.\n\n\nDeploy instances evenly across all available AZs, so that only a minimal fraction of your capacity is lost in case of an AZ outage.\n\n\nIf your architecture has single points of failure, put all of them into a single AZ. This may seem counter-intuitive, but it minimizes the likelihood of any one SPOF to go down on an outage of a single AZ.\n\n\n\n\n\n\nEBS vs instance storage:\n For a number of years, EBSs had a poorer track record for availability than instance storage. For systems where individual instances can be killed and restarted easily, instance storage with sufficient redundancy could give higher availability overall. EBS has improved, and modern instance types (since 2015) are now EBS-only, so this approach, while helpful at one time, may be increasingly archaic.\n\n\nBe sure to \nuse and understand CLBs/ALBs\n appropriately. Many outages are due to not using load balancers, or misunderstanding or misconfiguring them.", 
            "title": "Tips"
        }, 
        {
            "location": "/IOT/alternatives-and-lock-in/", 
            "text": "AWS, Microsoft and Google have all introduced IoT-specific sets of cloud services since late 2015. AWS was first, moving their IoT services to \ngeneral availability\n in Dec 2015. Microsoft released their set of IoT services for Azure in \nFeb 2016\n.  Google has only previewed, but not released their IoT services \nAndroid Things\n and \nWeave\n.\n\n\nIssues of lock-in center around your devices   \nprotocols\n (for example MQTT, AMQP), message formats (such as, JSON vs. Hex...) and security (certificates).", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/IOT/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n\n\nIoT\n is a platform for allowing clients such as IoT devices or software applications (\nexamples\n) to communicate with the AWS cloud.\n\n\nClients are also called \ndevices\n (or \nthings\n) and include a wide variety of device types.  Roughly there are three categories of device types that interact with IoT services by sending message over an IoT protocol to the IoT Pub/Sub-style message broker, which is called the IoT \nDevice Gateway\n:\n\n\nSend messages only: For example, the \nAWS IoT Button\n on an \neddystone beacon\n.\n\n\nSend and receive messages: For example, the \nPhillips Home Safe Medical Alert device\n\n\nSend, receive, and process messages: For example, a simple processing board, such as a \nRaspberry Pi\n (\nquick start guide\n), or an AWS device, such as \nEcho or Echo Dot\n, which are designed to work with the \nAWS Alexa skills kit\n (a programmable voice-enabled service from AWS).\n\n\n\n\n\n\nAWS has a useful \nquick-start\n (using the Console) and a \nslide presentation\n on core topics.\n\n\nIoT terms:\n\n\nAWS \nIoT Things\n (metadata for devices in a \nregistry\n) and can store device state in a JSON document, which is called a \ndevice shadow\n.  Device metadata can also be stored in \nIoT Thing Types\n. This aids in device metadata management by allowing for reuse of device description and configuration for more than one device.  Note that IoT Thing Types can be deprecated, but not changed  they are immutable.\n\n\nAWS \nIoT Certificates\n (device authentication) are the logical association of a unique certificate to the logical representation of a device. This association can be done in the Console.  In addition, the public key of the certificate must be copied to the physical device. This covers the authentication of devices to a particular AWS Device Gateway (or message broker). You can associate an AWS IoT certificate with an IoT device or you can \nregister your own CA (Certificate Authority) with AWS\n, generate your own certificate(s) and associate those certificates with your devices via the AWS Console or cli.\n\n\nAWS \nIoT Policies\n (device/topic authorization) are JSON files that are associated to one or more AWS IoT certificates. This authorizes associated devices to publish and/or subscribe to messages from one or more MQTT topics.\n\n\nAWS \nIoT Rules\n are SQL-like queries which allows for reuse of some or all device message data, as described in \nthis presentation, which summarizes design patterns with for IoT Rules\n.\n\n\nShown below is a \ndiagram\n which summarizes the flow of messages between the AWS IoT services:", 
            "title": "Basics"
        }, 
        {
            "location": "/IOT/code-samples/", 
            "text": "Simple Beer Service\n is a surprisingly useful code example using AWS IoT, Lambda, etc.\n\n\nIoT-elf\n offers clean Python sample using the AWS IoT SDK.\n\n\nIoT Button projects\n on Hackster include many different code samples for projects.\n\n\n5 IoT code examples\n: a device simulator, MQTT sample, just in time registration, truck simulator, prediction data simulator.\n\n\nAWS Alexa trivia voice example\n is a quick-start using Alexa voice capability and Lambda.\n\n\nSome Raspberry Pi examples include the \nBeacon project\n, \nDanbo\n, and \nGoPiGo\n.", 
            "title": "Code samples"
        }, 
        {
            "location": "/IOT/gotchas-and-limitations/", 
            "text": "IoT protocols:\n It is important to verify the exact type of support for your particular IoT device message protocol. For example, one commonly used IoT protocol is \nMQTT\n. Within MQTT there are \nthree possible levels of QoS in MQTT\n.  AWS IoT supports MQTT \nQoS 0\n (fire and forget, or at most once) and QoS 1(at least once, or includes confirmation), but \nnot\n QoS 2 (exactly once, requires 4-step confirmation).  This is important in understanding how much code youll need to write for your particular application message resolution needs.  Here is a \npresentation about the nuances of connecting\n.\n\n\nThe ecosystems to match \nIAM users or roles\n to \nIoT policies\n and their associated authorized AWS IoT devices are immature. Custom coding to enforce your security requirements is common.\n\n\nA common mistake is to misunderstand the importance of IoT \ndevice\n \nsecurity\n.  It is imperative to associate \neach\n device with a unique certificate (public key). You can generate your own certificates and upload them to AWS, or you can use AWS generated IoT device certificates. Its best to read and understand AWSs own guidance on this \ntopic\n.\n\n\nThere is only one \nAWS IoT Gateway\n (endpoint) per AWS account. For production scenarios, youll probably need to set up multiple AWS accounts in order to separate device traffic for development, test and production. Its interesting to note that the \nAzure IoT Gateway\n supports configuration of multiple endpoints, so that a single Azure account can be used with separate pub/sub endpoints for development, testing and production\n\n\nLimits:\n Be aware of \nlimits\n, including device message size, type, frequency, and number of AWS IoT rules.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/IOT/tips/", 
            "text": "Getting started with Buttons:\n One way to start is to use an \nAWS IoT Button\n.  AWS provides a number of code samples for use with their IoT Button, you can use the AWS IoT console, click the connect AWS IoT button link and you'll be taken to the  AWS Lambda console.  There you fill out your buttons serial number to associate it with a Lambda. (As of this writing, AWS IoT buttons are only available for sale in the US.)\n\n\nConnections and protocols:\n It is important to understand the details of about the devices you wish to connect to the AWS IoT service, including how you will secure the device connections, the device protocols, and more. Cloud vendors differ significantly in their support for common IoT protocols, such as MQTT, AMQP, XMPP. AWS IoT supports \nsecure MQTT\n, \nWebSockets\n and \nHTTPS\n.\n\n\nSupport for \ndevice security\n via certificate processing is a key differentiator in this space.  In August 2016, AWS added \njust-in-time registrations\n for IoT devices to their services.\n\n\nCombining with other services:\n Its common to use other AWS services, such as AWS Lambda, Kinesis and DynamoDB, although this is by no means required.  Sample IoT application reference architectures are in this \nscreencast\n.\n\n\nTesting tools:\n\n\nTo get started, AWS includes a lightweight MQTT client in the AWS IoT console. Here you can create and test sending and receiving messages to and from various MQTT topics.\n\n\nWhen testing locally, if using MQTT, it may be helpful to download and use the open source \nMosquitto broker\n tool for local testing with devices and/or device simulators\n\n\nUse this \nMQTT load simulator\n to test device message load throughout your IoT solution.", 
            "title": "Tips"
        }, 
        {
            "location": "/KMS/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nKMS\n (Key Management Service) is a secure service for creating, storing and auditing usage of cryptographic keys.\n\n\nService integration:\n KMS \nintegrates with other AWS services\n: EBS, Elastic Transcoder, EMR, Redshift, RDS, SES, S3, WorkMail and Workspaces.\n\n\nEncryption APIs:\n The \nEncrypt\n and \nDecrypt API\n allow you to encrypt and decrypt data on the KMS service side, never exposing the master key contents.\n\n\nData keys:\n The \nGenerateDataKey\n API generates a new key off of a master key. The data key contents are exposed to you so you can use it to encrypt and decrypt any size of data in your application layer. KMS does not store, manage or track data keys, you are responsible for this in your application.\n\n\nAuditing:\n Turn on CloudTrail to audit all KMS API events.\n\n\nAccess:\n Use \nkey policies\n and \nIAM policies\n to grant different levels of KMS access. For example, you create an IAM policy that only \nallows a user to encrypt and decrypt with a specific key\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/KMS/gotchas-and-limitations/", 
            "text": "The Encrypt API only works with \n 4KB of data. Larger data requires generating and managing a \ndata key\n in your application layer.\n\n\nKMS audit events are not available in the \nCloudTrail Lookup Events API\n. You need to look find them in the raw .json.gz files that CloudTrail saves in S3.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/KMS/tips/", 
            "text": "Its very common for companies to manage keys completely via home-grown mechanisms, but its far preferable to use a service such as KMS from the beginning, as it encourages more secure design and improves policies and processes around managing keys.\n\n\nA good motivation and overview is in \nthis AWS presentation\n.\n\n\nThe cryptographic details are in \nthis AWS whitepaper\n.\n\n\nThis blog from Convox\n demonstrates why and how to use KMS for encryption at rest.", 
            "title": "Tips"
        }, 
        {
            "location": "/Kinesis Streams/alternatives-and-lock-in/", 
            "text": "Kinesis is most closely compared to \nApache Kafka\n, an open-source data ingestion solution. It is possible to set up a Kafka cluster hosted on \nEC2 instances\n (or any other VPS), however you are responsible for managing and maintaining both Zookeeper and the Kafka brokers in a highly available configuration. Confluent has a good blog post with their recommendations on how to do this \nhere\n, which has links on the bottom to several other blogs they have written on the subject.\n\n\nKinesis uses very AWS-specific APIs, so you should be aware of the potential future costs of migrating away from it, should you choose to use it.\n\n\nAn application that efficiently uses Kinesis Streams will scale the number of shards up and down based on the required streaming capacity. (Note there is no direct equivalent to this with Apache Kafka.)", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/Kinesis Streams/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nKinesis Streams\n (which used to be only called Kinesis, before Kinesis Firehose and Kinesis Analytics were launched) is a service that allows you to ingest high-throughput data streams for immediate or delayed processing by other AWS services.\n\n\nKinesis Streams subcomponents are called \nshards\n. Each shard provides 1MB/s of write capacity and 2MB/s of read capacity at a maximum of 5 reads per second. A stream can have its shards programmatically increased or decreased based on a variety of metrics.\n\n\nAll records entered into a Kinesis Stream are assigned a unique sequence number as they are captured. The records in a Stream are ordered by this number, so any time-ordering is preserved.\n\n\nThis page\n summarizes key terms and concepts for Kinesis Streams.", 
            "title": "Basics"
        }, 
        {
            "location": "/Kinesis Streams/gotchas-and-limitations/", 
            "text": "Kinesis Streams shards each only permit \n5 reads per second\n. If you are evenly distributing data across many shards, your read limit for the Stream will remain at 5 reads per second on aggregate, as each consuming application will need to check every single shard for new records. This puts a hard limit on the number of different consuming applications possible per Stream for a given maximum read latency.\n\n\nFor example, if you have 5 consuming applications reading data from one Stream with any number of shards, they cannot read with a latency of less than one second, as each of the 5 consumers will need to poll \neach shard\n every second, reaching the cap of 5 reads per second per shard.\n\n\nThis blog post\n further discusses the performance and limitations of Kinesis in production.\n\n\n\n\n\n\nKinesis Streams are not included in the free tier.\n Make sure if you do any experimentation with it on a personal account, you shut down the stream or it may run up unexpected costs (~$11 per shard-month.)", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Kinesis Streams/tips/", 
            "text": "The \nKCL\n (Kinesis Client Library) provides a skeleton interface for Java, Node, Python, Ruby and .NET programs to easily consume data from a Kinesis Stream. In order to start consuming data from a Stream, you only need to provide a config file to point at the correct Kinesis Stream, and functions for initialising the consumer, processing the records, and shutting down the consumer within the skeletons provided.\n\n\nThe KCL uses a DynamoDB table to keep track of which records have been processed by the KCL. This ensures that all records are processed at least once. It is up to the developer to ensure that the program can handle doubly-processed records.\n\n\nThe KCL also uses DynamoDB to keep track of other KCL workers. It automatically shares the available Kinesis Shards across all the workers as equally as possible.", 
            "title": "Tips"
        }, 
        {
            "location": "/Lambda/alternatives-and-lock-in/", 
            "text": "Other clouds offer similar services with different names, including \nGoogle Cloud Functions\n, \nAzure Functions\n, and \nIBM OpenWhisk\n.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/Lambda/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nLambda\n is a relatively new service (launched at end of 2014) that offers a different type of compute abstraction: A user-defined function that can perform a small operation, where AWS manages provisioning and scheduling how it is run.", 
            "title": "Basics"
        }, 
        {
            "location": "/Lambda/code-samples/", 
            "text": "Fan-out\n is an example of using Lambda to fan-out or copy data from one service, in this case Kinesis, to multiple other AWS data services. Destinations for fan-out data in the sample include IoT, SQS and more.\n\n\nThis \nAWS limit monitor using Lambdas\n shows use of multiple Lambdas for monitoring.\n\n\nThis \nLambda ECS Worker Pattern\n shows use of Lambda in a workflow where data from S3 is picked up by the Lambda, pushed to a queue, then sent to ECS for more processing.\n\n\nThe \nSecure Pet Store\n is a sample Java application which uses Lambda and API Gateway with Cognito (for user identity).\n\n\n\n\nPlease help expand this incomplete section.", 
            "title": "Code samples"
        }, 
        {
            "location": "/Lambda/gotchas-and-limitations/", 
            "text": "Lambda is a new technology. As of mid 2016, only a few companies are using it for large-scale production applications.\n\n\nManaging lots of Lambda functions is a workflow challenge, and tooling to manage Lambda deployments is still immature.\n\n\nAWS official workflow around managing function \nversioning and aliases\n is painful.\n\n\nCurrently \nas of October, 2016\n Lambda functions can sometimes stop working for 2-3 minutes for failure recovery purposes according to a support ticket answer from Lambda development team. They are working to prevent this in the future.\n\n\nWhile adding/removing S3 buckets as triggers for Lambda function, this error may occur: \"There was an error creating the trigger: Configuration is ambiguously defined. Cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type.\" In this case, you can manually remove the Lambda event in the \"Events\" tab in the \"Properties\" section of the S3 bucket.\n\n\nAt the time of writing (12 December 2016) Dead Letter Queues are only available in the Ohio (us-east-2) region.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Lambda/tips/", 
            "text": "What does serverless mean?\n This idea of using Lambda for application logic has grown to be called \nserverless\n since you don't explicitly manage any server instances, as you would with EC2. This term is a bit confusing since the functions themselves do of course run on servers managed by AWS. \nServerless, Inc.\n also uses this word for the name of their company and \ntheir own open source framework\n, but the term is usually meant more generally.\n\n\nThe release of Lambda and \nAPI Gateway\n in 2015 triggered a startlingly rapid adoption in 2016, with many people writing about \nserverless architectures\n in which many applications traditionally solved by managing EC2 servers can be built without explicitly managing servers at all.\n\n\nFrameworks:\n \nSeveral frameworks\n for building and managing serverless deployment are emerging.\n\n\nThe \nAwesome Serverless\n list gives a good set of examples of the relatively new set of tools and frameworks around Lambda.\n\n\nThe \nServerless framework\n is a leading new approach designed to help group and manage Lambda functions. Its approaching version 1 as of August 2016) and is popular among a small number of users.", 
            "title": "Tips"
        }, 
        {
            "location": "/Learning And Career Development/certifications/", 
            "text": "Certifications:\n AWS offers \ncertifications\n for IT professionals who want to demonstrate their knowledge.\n\n\nCertified Solutions Architect Associate\n\n\nCertified Developer Associate\n\n\nCertified SysOps Administrator Associate\n\n\nCertified Solutions Architect Professional\n\n\nCertified DevOps Engineer Professional\n\n\nGetting certified:\n If youre interested in studying for and getting certifications, \nthis practical overview\n tells you a lot of what you need to know. The official page is \nhere\n and there is an \nFAQ\n.\n\n\nDo you need a certification?\n Especially in consulting companies or when working in key tech roles in large non-tech companies, certifications are important credentials. In others, including in many tech companies and startups, certifications are not common or considered necessary. (In fact, fairly or not, some Silicon Valley hiring managers and engineers see them as a negative signal on a resume.)", 
            "title": "Certifications"
        }, 
        {
            "location": "/Load Balancers/basics/", 
            "text": "AWS has 2 load balancing products - Classic Load Balancers (CLBs) and Application Load Balancers (ALBs).\n\n\nBefore the introduction of ALBs, Classic Load Balancers were known as Elastic Load Balancers (ELBs), so older documentation, tooling, and blog posts may still reference ELBs.\n\n\nCLBs have been around since 2009 while ALBs are a recent (2016) addition to AWS.\n\n\nCLBs support TCP and HTTP load balancing while ALBs support HTTP load balancing only.\n\n\nBoth can optionally handle termination for a single SSL certificate.\n\n\nBoth can optionally perform active health checks of instances and remove them from the destination pool if they become unhealthy.\n\n\nCLBs don't support complex / rule-based routing, while ALBs support a (currently small) set of rule-based routing features.\n\n\nCLBs can only forward traffic to a single globally configured port on destination instances, while ALBs can forward to ports that are configured on a per-instance basis, better supporting routing to services on shared clusters with dynamic port assignment (like ECS or Mesos).\n\n\nCLBs are supported in EC2 Classic as well as in VPCs while ALBs are supported in VPCs only.", 
            "title": "Basics"
        }, 
        {
            "location": "/Load Balancers/gotchas-and-limitations/", 
            "text": "CLBs and ALBs have \nno fixed external IP\n that all clients see. For most consumer apps this doesnt matter, but enterprise customers of yours may want this. IPs will be different for each user, and will vary unpredictably for a single client over time (within the standard \nEC2 IP ranges\n). And similarly, never resolve an CLB name to an IP and put it as the value of an A record  it will work for a while, then break!\n\n\nSome web clients or reverse proxies cache DNS lookups for a long time, which is problematic for CLBs and ALBs, since they change their IPs. This means after a few minutes, hours, or days, your client will stop working, unless you disable DNS caching. Watch out for \nJavas settings\n and be sure to \nadjust them properly\n. Another example is nginx as a reverse proxy, which \nnormally resolves backends only at start-up\n (although there is \na way to get around this\n).\n\n\nIts not unheard of for IPs to be recycled between customers without a long cool-off period. So as a client, if you cache an IP and are not using SSL (to verify the server), you might get not just errors, but responses from completely different services or companies!\n\n\nAs an operator of a service behind an CLB or ALB, the latter phenomenon means you can also see puzzling or erroneous requests by clients of other companies. This is most common with clients using back-end APIs (since web browsers typically cache for a limited period).\n\n\nCLBs and ALBs take time to scale up, it does not handle sudden spikes in traffic well. Therefore, if you anticipate a spike, you need to pre-warm the load balancer by gradually sending an increasing amount of traffic.\n\n\nTune your healthchecks carefully  if you are too aggressive about deciding when to remove an instance and conservative about adding it back into the pool, the service that your load balancer is fronting may become inaccessible for seconds or minutes at a time. Be extra careful about this when an autoscaler is configured to terminate instances that are marked as being unhealthy by a managed load balancer.\n\n\nCLB HTTPS listeners don't support Server Name Indication (SNI). If you need SNI, you can work around this limitation by either providing a certificate with Subject Alternative Names (SANs) or by using TCP listeners and terminating SSL at your backend.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Load Balancers/tips/", 
            "text": "If you dont have opinions on your load balancing up front, and dont have complex load balancing needs like application-specific routing of requests, its reasonable just to use an CLB or ALB for load balancing instead.\n\n\nEven if you dont want to think about load balancing at all, because your architecture is so simple (say, just one server), put a load balancer in front of it anyway. This gives you more flexibility when upgrading, since you wont have to change any DNS settings that will be slow to propagate, and also it lets you do a few things like terminate SSL more easily.\n\n\nCLBs and ALBs have many IPs:\n Internally, an AWS load balancer is simply a collection of individual software load balancers hosted within EC2, with DNS load balancing traffic among them. The pool can contain many IPs, at least one per availability zone, and depending on traffic levels. They also support SSL termination, which is very convenient.\n\n\nScaling:\n CLBs and ALBs can scale to very high throughput, but scaling up is not instantaneous. If youre expecting to be hit with a lot of traffic suddenly, it can make sense to load test them so they scale up in advance. You can also \ncontact Amazon\n and have them pre-warm the load balancer.\n\n\nClient IPs:\n In general, if servers want to know true client IP addresses, load balancers must forward this information somehow. CLBs add the standard \nX-Forwarded-For\n header. When using an CLB as an HTTP load balancer, its possible to get the clients IP address from this.\n\n\nUsing load balancers when deploying:\n One common pattern is to swap instances in the load balancer after spinning up a new stack with your latest version, keep old stack running for one or two hours, and either flip back to old stack in case of problems or tear it down.", 
            "title": "Tips"
        }, 
        {
            "location": "/Managing AWS/apis-and-sdks/", 
            "text": "SDKs\n for using AWS APIs are available in most major languages, with \nGo\n, \niOS\n, \nJava\n, \nJavaScript\n, \nPython\n, \nRuby\n, and \nPHP\n being most heavily used. AWS maintains \na short list\n, but the \nawesome-aws list\n is the most comprehensive and current. Note \nsupport for C++\n is \nstill new\n.\n\n\nRetry logic:\n An important aspect to consider whenever using SDKs is error handling; under heavy use, a wide variety of failures, from programming errors to throttling to AWS-related outages or failures, can be expected to occur. SDKs typically implement \nexponential backoff\n to address this, but this may need to be understood and adjusted over time for some applications. For example, it is often helpful to alert on some error codes and not on others.\n\n\nDont use APIs directly. Although AWS documentation includes lots of API details, its better to use the SDKs for your preferred language to access APIs. SDKs are more mature, robust, and well-maintained than something youd write yourself.", 
            "title": "Apis and sdks"
        }, 
        {
            "location": "/Managing AWS/aws-configuration-management/", 
            "text": "The first way most people experiment with AWS is via its web interface, the AWS Console. But using the Console is a highly manual process, and often works against automation or flexibility.\n\n\nSo if youre not going to manage your AWS configurations manually, what should you do? Sadly, there are no simple, universal answers  each approach has pros and cons, and the approaches taken by different companies vary widely, and include directly using APIs (and building tooling on top yourself), using command-line tools, and using third-party tools and services.", 
            "title": "Aws configuration management"
        }, 
        {
            "location": "/Managing AWS/aws-console/", 
            "text": "The \nAWS Console\n lets you control much (but not all) functionality of AWS via a web interface.\n\n\nIdeally, you should only use the AWS Console in a few specific situations:\n\n\nIts great for read-only usage. If youre trying to understand the state of your system, logging in and browsing it is very helpful.\n\n\nIt is also reasonably workable for very small systems and teams (for example, one engineer setting up one server that doesnt change often).\n\n\nIt can be useful for operations youre only going to do rarely, like less than once a month (for example, a one-time VPC setup you probably wont revisit for a year). In this case using the console can be the simplest approach.\n\n\n\n\n\n\nThink before you use the console:\n The AWS Console is convenient, but also the enemy of automation, reproducibility, and team communication. If youre likely to be making the same change multiple times, avoid the console. Favor some sort of automation, or at least have a path toward automation, as discussed next. Not only does using the console preclude automation, which wastes time later, but it prevents documentation, clarity, and standardization around processes for yourself and your team.", 
            "title": "Aws console"
        }, 
        {
            "location": "/Managing AWS/boto/", 
            "text": "A good way to automate operations in a custom way is \nBoto3\n, also known as the \nAmazon SDK for Python\n. \nBoto2\n, the previous version of this library, has been in wide use for years, but now there is a newer version with official support from Amazon, so prefer Boto3 for new projects.\n\n\nIf you find yourself writing a Bash script with more than one or two CLI commands, youre probably doing it wrong. Stop, and consider writing a Boto script instead. This has the advantages that you can:\n\n\nCheck return codes easily so success of each step depends on success of past steps.\n\n\nGrab interesting bits of data from responses, like instance ids or DNS names.\n\n\nAdd useful environment information (for example, tag your instances with git revisions, or inject the latest build identifier into your initialization script).", 
            "title": "Boto"
        }, 
        {
            "location": "/Managing AWS/command-line-tools/", 
            "text": "The \naws command-line interface\n (CLI), used via the \naws\n command, is the most basic way to save and automate AWS operations.\n\n\nDont underestimate its power. It also has the advantage of being well-maintained  it covers a large proportion of all AWS services, and is up to date.\n\n\nIn general, whenever you can, prefer the command line to the AWS Console for performing operations.\n\n\nEven in absence of fancier tools, you can \nwrite simple Bash scripts\n that invoke \naws\n with specific arguments, and check these into Git. This is a primitive but effective way to document operations youve performed. It improves automation, allows code review and sharing on a team, and gives others a starting point for future work.\n\n\nFor use that is primarily interactive, and not scripted, consider instead using the \naws-shell\n tool from AWS. It is easier to use, with auto-completion and a colorful UI, but still works on the command line. If youre using \nSAWS\n, a previous version of the program, \nyou should migrate to aws-shell\n.", 
            "title": "Command line tools"
        }, 
        {
            "location": "/Managing AWS/general-visibility/", 
            "text": "Tagging resources\n is an essential practice, especially as organizations grow, to better understand your resource usage. For example, through automation or convention, you can add tags:\n\n\nFor the org or developer that owns that resource\n\n\nFor the product that resource supports\n\n\nTo label lifecycles, such as temporary resources or one that should be deprovisioned in the future\n\n\nTo distinguish production-critical infrastructure (e.g. serving systems vs backend pipelines)\n\n\nTo distinguish resources with special security or compliance requirements", 
            "title": "General visibility"
        }, 
        {
            "location": "/Managing AWS/managing-infrastructure-state-and-change/", 
            "text": "A great challenge in using AWS to build complex systems (and with DevOps in general) is to manage infrastructure state effectively over time. In general, this boils down to three broad goals for the state of your infrastructure:\n\n\n\n\nVisibility\n: Do you know the state of your infrastructure (what services you are using, and exactly how)? Do you also know when you  and anyone on your team  make changes? Can you detect misconfigurations, problems, and incidents with your service?\n\n\nAutomation\n: Can you reconfigure your infrastructure to reproduce past configurations or scale up existing ones without a lot of extra manual work, or requiring knowledge thats only in someones head? Can you respond to incidents easily or automatically?\n\n\nFlexibility\n: Can you improve your configurations and scale up in new ways without significant effort? Can you add more complexity using the same tools? Do you share, review, and improve your configurations within your team?\n\n\n\n\nMuch of what we discuss below is really about how to improve the answers to these questions.\n\n\nThere are several approaches to deploying infrastructure with AWS, from the console to complex automation tools, to third-party services, all of which attempt to help achieve visibility, automation, and flexibility.", 
            "title": "Managing infrastructure state and change"
        }, 
        {
            "location": "/Managing Servers And Applications/aws-vs-server-configuration/", 
            "text": "This guide is about AWS, not DevOps or server configuration management in general. But before getting into AWS in detail, its worth noting that in addition to the configuration management for your AWS resources, there is the long-standing problem of configuration management for servers themselves.", 
            "title": "Aws vs server configuration"
        }, 
        {
            "location": "/Managing Servers And Applications/containers-and-aws/", 
            "text": "Docker\n and the containerization trend are changing the way many servers and services are deployed in general.\n\n\nContainers are designed as a way to package up your application(s) and all of their dependencies in a known way. When you build a container, you are including every library or binary your application needs, outside of the kernel. A big advantage of this approach is that its easy to test and validate a container locally without worrying about some difference between your computer and the servers you deploy on.\n\n\nA consequence of this is that you need fewer AMIs and boot scripts; for most deployments, the only boot script you need is a template that fetches an exported docker image and runs it.\n\n\nCompanies that are embracing \nmicroservice architectures\n will often turn to container-based deployments.\n\n\nAWS launched \nECS\n as a service to manage clusters via Docker in late 2014, though many people still deploy Docker directly themselves. See the \nECS section\n for more details.", 
            "title": "Containers and aws"
        }, 
        {
            "location": "/Managing Servers And Applications/philosophy/", 
            "text": "Herokus \nTwelve-Factor App\n principles list some established general best practices for deploying applications.\n\n\nPets vs cattle:\n Treat servers \nlike cattle, not pets\n. That is, design systems so infrastructure is disposable. It should be minimally worrisome if a server is unexpectedly destroyed.\n\n\nThe concept of \nimmutable infrastructure\n is an extension of this idea.\n\n\nMinimize application state on EC2 instances. In general, instances should be able to be killed or die unexpectedly with minimal impact. State that is in your application should quickly move to RDS, S3, DynamoDB, EFS, or other data stores not on that instance. EBS is also an option, though it generally should not be the bootable volume, and EBS will require manual or automated re-mounting.", 
            "title": "Philosophy"
        }, 
        {
            "location": "/Managing Servers And Applications/server-configuration-management/", 
            "text": "There is a \nlarge set\n of open source tools for managing configuration of server instances.\n\n\nThese are generally not dependent on any particular cloud infrastructure, and work with any variety of Linux (or in many cases, a variety of operating systems).\n\n\nLeading configuration management tools are \nPuppet\n, \nChef\n, \nAnsible\n, and \nSaltstack\n. These arent the focus of this guide, but we may mention them as they relate to AWS.", 
            "title": "Server configuration management"
        }, 
        {
            "location": "/Managing Servers And Applications/tips-for-managing-servers/", 
            "text": "Timezone settings on servers\n: unless \nabsolutely necessary\n, always \nset the timezone on servers to \nUTC\n (see instructions for your distribution, such as \nUbuntu\n, \nCentOS\n or \nAmazon\n Linux). Numerous distributed systems rely on time for synchronization and coordination and UTC \nprovides\n the universal reference plane: it is not subject to  daylight savings changes and adjustments in local time. It will also save you a lot of headache debugging \nelusive timezone issues\n and provide coherent timeline of events in your logging and audit systems.\n\n\nNTP and accurate time:\n If you are not using Amazon Linux (which comes preconfigured), you should confirm your servers \nconfigure NTP correctly\n, to avoid insidious time drift (which can then cause all sorts of issues, from breaking API calls to misleading logs). This should be part of your automatic configuration for every server. If time has already drifted substantially (generally \n1000 seconds), remember NTP wont shift it back, so you may need to remediate manually (for example, \nlike this\n on Ubuntu).\n\n\nTesting immutable infrastructure:\n If you want to be proactive about testing your services ability to cope with instance termination or failure, it can be helpful to introduce random instance termination during business hours, which will expose any such issues at a time when engineers are available to identify and fix them. Netflixs \nSimian Army\n (specifically, \nChaos Monkey\n) is a popular tool for this. Alternatively, \nchaos-lambda\n by the BBC is a lightweight option which runs on AWS \nLambda\n.", 
            "title": "Tips for managing servers"
        }, 
        {
            "location": "/Managing Servers And Applications/visibility/", 
            "text": "Store and track instance metadata (such as instance id, availability zone, etc.) and deployment info (application build id, Git revision, etc.) in your logs or reports. The \ninstance metadata service\n can help collect some of the AWS data youll need.\n\n\nUse log management services:\n Be sure to set up a way to view and manage logs externally from servers.\n\n\nCloud-based services such as \nSumo Logic\n, \nSplunk Cloud\n, \nScalyr\n, and \nLoggly\n are the easiest to set up and use (and also the most expensive, which may be a factor depending on how much log data you have).\n\n\nMajor open source alternatives include \nElasticsearch\n, \nLogstash\n, and \nKibana\n (the \nElastic Stack\n) and \nGraylog\n.\n\n\nIf you can afford it (you have little data or lots of money) and dont have special needs, it makes sense to use hosted services whenever possible, since setting up your own scalable log processing systems is notoriously time consuming.\n\n\n\n\n\n\nTrack and graph metrics:\n The AWS Console can show you simple graphs from CloudWatch, you typically will want to track and graph many kinds of metrics, from CloudWatch and your applications. Collect and export helpful metrics everywhere you can (and as long as volume is manageable enough you can afford it).\n\n\nServices like \nLibrato\n, \nKeenIO\n, and \nDatadog\n have fancier features or better user interfaces that can save a lot of time. (A more detailed comparison is \nhere\n.)\n\n\nUse \nPrometheus\n or \nGraphite\n as timeseries databases for your metrics (both are open source).\n\n\nGrafana\n can visualize with dashboards the stored metrics of both timeseries databases (also open source).", 
            "title": "Visibility"
        }, 
        {
            "location": "/RDS/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nPricing\n (see also \nec2instances.info/rds/\n)\n\n\nRDS\n is a managed relational database service, allowing you to deploy and scale databases more easily. It supports \nOracle\n, \nMicrosoft SQL Server\n, \nPostgreSQL\n, \nMySQL\n, \nMariaDB\n, and Amazons own \nAurora\n.\n\n\nRDS offers out of the box support for \nhigh availability and failover\n for your databases.", 
            "title": "Basics"
        }, 
        {
            "location": "/RDS/gotchas-and-limitations/", 
            "text": "RDS instances run on EBS volumes (either general-purpose or provisioned IOPS), and hence are constrained by EBS performance.\n\n\nVerify what database features you need, as not everything you might want is available on RDS. For example, if you are using Postgres, check the list of \nsupported features and extensions\n. If the features you need aren't supported by RDS, you'll have to deploy your database yourself.\n\n\nIf you use the failover support offered by RDS, keep in mind that it is based on DNS changes, and make sure that your client reacts to these changes appropriately. This is particularly important for Java, given how its DNS resolvers TTL is \nconfigured by default\n.\n\n\nDB migration to RDS:\n While importing your database into RDS ensure you take into consideration the maintenance window settings. If a backup is running at the same time, your import can take a considerably longer time than you would have expected.\n\n\nDatabase sizes are limited\n to \n6TB\n for all database engines except for SQL Server which has a \n4TB\n limit and Aurora which supports up to \n64TB\n databases.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/RDS/mysql-and-mariadb-basics/", 
            "text": "RDS offers MySQL versions 5.5, 5.6, and 5.7.", 
            "title": "Mysql and mariadb basics"
        }, 
        {
            "location": "/RDS/mysql-and-mariadb-gotchas-and-limitations/", 
            "text": "No SUPER privileges.\n RDS provides some \nstored procedures\n to perform some tasks that require SUPER privileges such as starting or stopping replication.\n\n\nYou can replicate to non-RDS instances of MySQL, but \nreplication to these instances will break during AZ failovers\n.\n\n\nThere is no ability to manually CHANGE MASTER on replicas, so they must all be rebuilt after a failover of the master.", 
            "title": "Mysql and mariadb gotchas and limitations"
        }, 
        {
            "location": "/RDS/mysql-and-mariadb-tips/", 
            "text": "MySQL RDS allows access to \nbinary logs\n.\n\n\nMulti-AZ instances of MySQL transparently replicate data across AZs using DRBD. Automated backups of multi-AZ instances \nrun off the backup instance\n to reduce latency spikes on the primary.\n\n\nMySQL vs MariaDB vs Aurora:\n If you prefer a MySQL-style database but are starting something new, you probably should consider Aurora and MariaDB as well. \nAurora\n has increased availability and is the next-generation solution. That said, Aurora \nmay not be\n as fast relative to MySQL as is sometimes reported, and is more complex to administer. \nMariaDB\n, the modern \ncommunity fork\n of MySQL, \nlikely now has the edge over MySQL\n for many purposes and is supported by RDS.", 
            "title": "Mysql and mariadb tips"
        }, 
        {
            "location": "/RDS/sql-server-basics/", 
            "text": "RDS offers SQL Server 2008 R2, 2012, and 2014\n including Express, Web, Standard and Enterprise (2008 R2 and 2012 only for Enterprise)", 
            "title": "Sql server basics"
        }, 
        {
            "location": "/RDS/sql-server-gotchas-and-limitations/", 
            "text": "The user is granted only db_owner privileges for each database on the instance.\n\n\nStorage cannot be expanded for existing databases. If you need more space, you must restore your database on a new instance with larger storage.\n\n\nThere is a \n4TB\n database size limit for non-Express editions.\n\n\nLimited to \n30 databases per instance", 
            "title": "Sql server gotchas and limitations"
        }, 
        {
            "location": "/RDS/sql-server-tips/", 
            "text": "Recently added support for \nbackup and restore to/from S3\n which may make it an attractive DR option for on-premises installations.", 
            "title": "Sql server tips"
        }, 
        {
            "location": "/RDS/tips/", 
            "text": "If you're looking for the managed convenience of RDS for other data stores such as MongoDB or Cassandra, you may wish to consider third-party services from providers such as \nmLab\n, \nCompose\n, or \nInstaClustr\n.\n\n\nMake sure to create a new \nparameter group\n and option group for your database since the default parameter group does not allow dynamic configuration changes.\n\n\nRDS instances start with a default timezone of UTC. If necessary, this can be \nchanged to a different timezone\n.", 
            "title": "Tips"
        }, 
        {
            "location": "/Redshift/alternatives-and-lock-in/", 
            "text": "Whatever data warehouse you select, your business will likely be locked in for a long time. Also (and not coincidentally) the data warehouse market is highly fragmented. Selecting a data warehouse is a choice to be made carefully, with research and awareness of \nthe market landscape\n and what \nbusiness intelligence\n tools youll be using.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/Redshift/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nRedshift\n is AWS managed \ndata warehouse\n solution, which is massively parallel, scalable, and columnar. It is very widely used. It \nwas built\n using \nParAccel\n technology and exposes \nPostgres\n-compatible interfaces.", 
            "title": "Basics"
        }, 
        {
            "location": "/Redshift/gotchas-and-limitations/", 
            "text": "While Redshift can handle heavy queries well, it does not scale horizontally, i.e. does not handle multiple queries in parallel. Therefore, if you expect a high parallel load, consider replicating or (if possible) sharding your data across multiple clusters.\n\n\nThe leader node, which manages communications with client programs and all communication with compute nodes, is the single point of failure.\n\n\nAlthough most Redshift queries parallelize well at the compute node level, certain stages are executed on the leader node, which can become the bottleneck.\n\n\nRedshift data commit transactions are very expensive and serialized at the cluster level. Therefore, consider grouping multiple mutation commands (COPY/INSERT/UPDATE) commands into a single transaction whenever possible.\n\n\nRedshift does not support multi-AZ deployments. Building multi-AZ clusters is not trivial. \nHere\n is an example using Kinesis.\n\n\nBeware of storing multiple small tables in Redshift. The way Redshift tables are laid out on disk makes it impractical. The minimum space required to store a table (in MB) is nodes * slices/node * columns. For example, on a 16 node cluster an empty table with 20 columns will occupy 640MB on disk.\n\n\nQuery performance degrades significantly during data ingestion. \nWLM (Workload Management)\n tweaks help to some extent. However, if you need consistent read performance, consider having replica clusters (at the extra cost) and swap them during update.\n\n\nNever resize a live cluster. The resize operation takes hours depending on the dataset size. In rare cases, the operation may also get stuck and you'll end up having a non-functional cluster. The safer approach is to create a new cluster from a snapshot, resize the new cluster and shut down the old one.\n\n\nRedshift has \nreserved keywords\n that are not present in Postgres (see full list \nhere\n). Watch out for DELTA (\nDelta Encodings\n).\n\n\nRedshift does not support many Postgres functions, most notably several date/time-related and aggregation functions. See the \nfull list here\n.\n\n\nCompression on sort key \ncan result in significant performance impact\n. So if your Redshift queries involving sort key(s) are slow, you might want to consider removing compression on a sort key.\n\n\nChoosing a sort key\n is very important since you can not change a tables sort key after it is created. If you need to change the sort or distribution key of a table, you need to create a new table with the new key and move your data into it with a query like insert into new_table select * from old_table.\n\n\nWhen moving data with a query that looks like insert into x select from y, you need to have twice as much disk space available as table y takes up on the clusters disks. Redshift first copies the data to disk and then to the new table. \nHere\n is a good article on how to this for big tables.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/Redshift/tips/", 
            "text": "Although Redshift is mostly Postgres-compatible, its SQL dialect and performance profile are different.\n\n\nRedshift supports only \n12 primitive data types\n. (\nList of unsupported Postgres types\n)\n\n\nIt has a leader node and computation nodes (the leader node distributes queries to the computation ones). Note that some functions \ncan be executed only on the lead node.\n\n\nMake sure to create a new \ncluster parameter group\n and option group for your database since the default parameter group does not allow dynamic configuration changes.\n\n\nMajor third-party BI tools support Redshift integration (see \nQuora\n).\n\n\nTop 10 Performance Tuning Techniques for Amazon Redshift\n provides an excellent list of performance tuning techniques.\n\n\nAmazon Redshift Utils\n contains useful utilities, scripts and views to simplify Redshift ops.\n\n\nVACUUM\n regularly following a significant number of deletes or updates to reclaim space and improve query performance.\n\n\nAvoid performing blanket \nVACUUM\n or \nANALYZE\n operations at a cluster level. The checks on each table to determine whether VACUUM or ANALYZE action needs to be taken is wasteful. Only perform ANALYZE and VACUUM commands on the objects that require it. Utilize the \nAnalyze \n Vacuum Schema Utility\n to perform this work. The SQL to determine whether a table needs to be VACUUMed or ANALYZEd can be found in the \nSchema Utility README\n if you wish to create your own maintenance process.\n\n\nRedshift provides various \ncolumn compression\n options to optimize the stored data size. AWS strongly encourages users to use \nautomatic compression\n at the COPY stage, when Redshift uses a sample of the data being ingested to analyze the column compression options. However, automatic compression can only be applied to an empty table with no data. Therefore, make sure the initial load batch is big enough to provide Redshift with a representative sample of the data (the default sample size is 100000 rows).\n\n\nRedshift uses columnar storage, hence it does not have indexing capabilities. You can, however, use distribution key \ndistkey\n and sort key \nsortkey\n to improve performance. Redshift has two type of sort keys: compounding sort key and interleaved sort key.\n\n\nA compound sort key is made up of all columns listed in the sort key definition. It is most useful when you have queries with operations using prefix of the sortkey.\n\n\nAn interleaved sort key on the other hand gives equal weight to each column or a subset of columns in the sort key. So if you don't know ahead of time which column you want to choose for sorting and filtering, this is a much better choice than the compound key.\nHere\n is an example using interleaved sort key.\n\n\nDistribution strategies:\n Since data in Redshift is physically distributed among nodes, choosing the right data \ndistribution key\n and \ndistribution style\n is crucial for adequate query performance. There are three possible distribution style settings  \nEVEN\n (the default), \nKEY\n, or \nALL\n. Use KEY to collocate join key columns for tables which are joined in queries. Use ALL to place the data in small-sized tables on all cluster nodes.", 
            "title": "Tips"
        }, 
        {
            "location": "/Route53/alternatives-and-lock-in/", 
            "text": "Historically, AWS was slow to penetrate the DNS market (as it is often driven by perceived reliability and long-term vendor relationships) but Route 53 has matured and \nis becoming the standard option\n for many companies. Route 53 is cheap by historic DNS standards, as it has a fairly large global network with geographic DNS and other formerly premium features. Its convenient if you are already using AWS.\n\n\nGenerally you dont get locked into a DNS provider for simple use cases, but increasingly become tied in once you use specific features like geographic routing or Route 53s alias records.\n\n\nMany alternative DNS providers exist, ranging from long-standing premium brands like \nUltraDNS\n and \nDyn\n to less well known, more modestly priced brands like \nDNSMadeEasy\n. Most DNS experts will tell you that the market is opaque enough that reliability and performance dont really correlate well with price.\n\n\nRoute 53 is usually somewhere in the middle of the pack on performance tests, e.g. the \nSolveDNS reports\n.", 
            "title": "Alternatives and lock in"
        }, 
        {
            "location": "/Route53/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nRoute 53\n is AWS DNS service.", 
            "title": "Basics"
        }, 
        {
            "location": "/Route53/tips/", 
            "text": "Know about Route 53s alias records:\n\n\nRoute 53 supports all the standard DNS record types, but note that \nalias resource record sets\n are not standard part of DNS, but a specific Route 53 feature. (Its available from other DNS providers too, but each provider has a different name for it.)\n\n\nAliases are like an internal name (a bit like a CNAME) that is resolved internally on the server side. For example, traditionally you could have a CNAME to the DNS name of a CLB or ALB, but its often better to make an alias to the same load balancer. The effect is the same, but in the latter case, externally, all a client sees is the target the record points to.\n\n\nIts often wise to use alias record as an alternative to CNAMEs, since they can be updated instantly with an API call, without worrying about DNS propagation.\n\n\nYou can use them for CLBs/ALBs or any other resource where AWS supports it.\n\n\nSomewhat confusingly, you can have CNAME and A aliases, depending on the type of the target.\n\n\nBecause aliases are extensions to regular DNS records, if exported, the output \nzone file\n will have additional non-standard ALIAS lines in it.\n\n\n\n\n\n\nLatency-based routing\n allows users around the globe to be automatically directed to the nearest AWS region where you are running, so that latency is reduced.\n\n\nUnderstand that domain registration and DNS management (hosted zones) are two separate Route 53 services. When you buy/transfer a domain, Route 53 automaticaly assigns four name servers to it (e.g. ns-2.awsdns-00.com). Route 53 also offers to automatically create a hosted zone for DNS management, but you are not required do your DNS management in the same account or even in Route 53; you just need to create an NS record pointing to the servers assigned to your domain in Route 53.\n\n\nOne use case would be to put your domain registration (very mission critical) in a \nbastion account\n while managing the hosted zones within another account which is accessible by your applications.", 
            "title": "Tips"
        }, 
        {
            "location": "/S3/basics/", 
            "text": "Homepage\n  \nDeveloper guide\n  \nFAQ\n  \nPricing\n\n\nS3\n (Simple Storage Service) is AWS standard cloud storage service, offering file (opaque blob) storage of arbitrary numbers of files of almost any size, from 0 to \n5TB\n. (Prior to \n2011\n the maximum size was 5 GB; larger sizes are now well supported via \nmultipart support\n.)\n\n\nItems, or \nobjects\n, are placed into named \nbuckets\n stored with names which are usually called \nkeys\n. The main content is the \nvalue\n.\n\n\nObjects are created, deleted, or updated. Large objects can be streamed, but you cannot access or modify parts of a value; you need to update the whole object.\n\n\nEvery object also has \nmetadata\n, which includes arbitrary key-value pairs, and is used in a way similar to HTTP headers. Some metadata is system-defined, some are significant when serving HTTP content from buckets or CloudFront, and you can also define arbitrary metadata for your own use.\n\n\nS3 URIs:\n Although often bucket and key names are provided in APIs individually, its also common practice to write an S3 location in the form 's3://bucket-name/path/to/key' (where the key here is 'path/to/key'). (Youll also see 's3n://' and 's3a://' prefixes \nin Hadoop systems\n.)\n\n\nS3 vs Glacier, EBS, and EFS:\n AWS offers many storage services, and several besides S3 offer file-type abstractions. \nGlacier\n is for cheaper and infrequently accessed archival storage. \nEBS\n, unlike S3, allows random access to file contents via a traditional filesystem, but can only be attached to one EC2 instance at a time. \nEFS\n is a network filesystem many instances can connect to, but at higher cost. See the \ncomparison table\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/S3/gotchas-and-limitations/", 
            "text": "For many years, there was a notorious \n100-bucket limit\n per account, which could not be raised and caused many companies significant pain. As of 2015, you can \nrequest increases\n. You can ask to increase the limit, but it will still be capped (generally below ~1000 per account).\n\n\nBe careful not to make implicit assumptions about transactionality or sequencing of updates to objects. Never assume that if you modify a sequence of objects, the clients will see the same modifications in the same sequence, or if you upload a whole bunch of files, that they will all appear at once to all clients.\n\n\nS3 has an \nSLA\n with 99.9% uptime. If you use S3 heavily, youll inevitably see occasional error accessing or storing data as disks or other infrastructure fail. Availability is usually restored in seconds or minutes. Although availability is not extremely high, as mentioned above, durability is excellent.\n\n\nAfter uploading, any change that you make to the object causes a full rewrite of the object, so avoid appending-like behavior with regular files.\n\n\nEventual data consistency, as discussed above, can be surprising sometimes. If S3 suffers from internal replication issues, an object may be visible from a subset of the machines, depending on which S3 endpoint they hit. Those usually resolve within seconds; however, weve seen isolated cases when the issue lingered for 20-30 hours.\n\n\nMD5s and multi-part uploads:\n In S3, the \nETag header in S3\n is a hash on the object. And in many cases, it is the MD5 hash. However, this \nis not the case in general\n when you use multi-part uploads. One workaround is to compute MD5s yourself and put them in a custom header (such as is done by \ns4cmd\n).\n\n\nIncomplete multi-part upload costs:\n Incomplete multi-part uploads accrue \nstorage charges\n even if the upload fails and no S3 object is created. \nAmazon\n (\nand\n \nothers\n) recommend using a lifecycle policy to clean up incomplete uploads and save on storage costs.\n\n\nUS Standard region:\n Previously, the us-east-1 region (also known as the US Standard region) was replicated across coasts, which led to greater variability of latency. Effective Jun 19, 2015 this is \nno longer the case\n. All Amazon S3 regions now support read-after-write consistency. Amazon S3 also renamed the US Standard region to the US East (N. Virginia) region to be consistent with AWS regional naming conventions.\n\n\nWhen configuring ACLs on who can access the bucket and contents, a predefined group exists called \nAuthenticated Users\n. This group is often used, incorrectly, to restrict S3 resource access to authenticated users of the owning account. If granted, the AuthenticatedUsers group will allow S3 resource access to \nall authenticated users, across all AWS accounts\n. A typical use case of this ACL is used in conjunction with the \nrequester pays\n functionality of S3.\n\n\nS3 authentication versions and regions:\n In newer regions, S3 \nonly supports the latest authentication\n. If an S3 file operation using CLI or SDK doesn't work in one region, but works correctly in another region, make sure you are using the latest \nauthentication signature\n.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/S3/storage-durability-availability-and-price/", 
            "text": "As an illustration of comparative features and price, the table below gives S3 Standard, RRS, IA, in comparison with \nGlacier\n, \nEBS\n, and \nEFS\n, using \nVirginia region\n as of \nAugust 2016\n.\n\n\n\n\n\n\n\n\n\n\nDurability (per year)\n\n\nAvailability designed\n\n\nAvailability SLA\n\n\nStorage (per TB per month)\n\n\nGET or retrieve (per million)\n\n\nWrite or archive (per million)\n\n\n\n\n\n\n\n\n\n\nGlacier\n\n\nEleven 9s\n\n\nSloooow\n\n\n\n\n$7\n\n\n$50\n\n\n$50\n\n\n\n\n\n\nS3 IA\n\n\nEleven 9s\n\n\n99.9%\n\n\n99%\n\n\n$12.50\n\n\n$1\n\n\n$10\n\n\n\n\n\n\nS3 RRS\n\n\n99.99%\n\n\n99.99%\n\n\n99.9%\n\n\n$24\n\n\n$0.40\n\n\n$5\n\n\n\n\n\n\nS3 Standard\n\n\nEleven 9s\n\n\n99.99%\n\n\n99.9%\n\n\n$30\n\n\n$0.40\n\n\n$5\n\n\n\n\n\n\nEBS\n\n\n99.8%\n\n\nUnstated\n\n\n99.95%\n\n\n$25/$45/\n$100\n/$125+ (\nsc1/st1/\ngp2\n/io1\n)\n\n\n\n\n\n\n\n\n\n\nEFS\n\n\nHigh\n\n\nHigh\n\n\n\n\n$300\n\n\n\n\n\n\n\n\n\n\n\n\nEspecially notable items are in \nboldface\n. Sources: \nS3 pricing\n, \nS3 SLA\n, \nS3 FAQ\n, \nRRS info\n, \nGlacier pricing\n, \nEBS availability and durability\n, \nEBS pricing\n, \nEFS pricing\n, \nEC2 SLA", 
            "title": "Storage durability availability and price"
        }, 
        {
            "location": "/S3/tips/", 
            "text": "For most practical purposes, you can consider S3 capacity unlimited, both in total size of files and number of objects. The number of objects in a bucket is essentially also unlimited. Customers routinely have millions of objects.\n\n\nBucket naming:\n Buckets are chosen from a global namespace (across all regions, even though S3 itself stores data in \nwhichever S3 region\n you select), so youll find many bucket names are already taken. Creating a bucket means taking ownership of the name until you delete it. Bucket names have \na few restrictions\n on them.\n\n\nBucket names can be used as part of the hostname when accessing the bucket or its contents, like \nbucket_name\n.s3-us-east-1.amazonaws.com\n, as long as the name is \nDNS compliant\n.\n\n\nA common practice is to use the company name acronym or abbreviation to prefix (or suffix, if you prefer DNS-style hierarchy) all bucket names (but please, dont use a check on this as a security measure  this is highly insecure and easily circumvented!).\n\n\nBucket names with '.' (periods) in them \ncan cause certificate mismatches\n when used with SSL. Use '-' instead, since this then conforms with both SSL expectations and is DNS compliant.\n\n\n\n\n\n\nVersioning:\n S3 has \noptional versioning support\n, so that all versions of objects are preserved on a bucket. This is mostly useful if you want an archive of changes or the ability to back out mistakes (it has none of the features of full version control systems like Git).\n\n\nDurability:\n Durability of S3 is extremely high, since internally it keeps several replicas. If you dont delete it by accident, you can count on S3 not losing your data. (AWS offers the seemingly improbable durability rate of \n99.999999999%\n, but this is a mathematical calculation based on independent failure rates and levels of replication  not a true probability estimate. Either way, S3 has had \na very good record\n of durability.) Note this is \nmuch\n higher durability than EBS! If durability is less important for your application, you can use \nS3 Reduced Redundancy Storage\n, which lowers the cost per GB, as well as the redundancy.\n\n\nS3 pricing\n depends on \nstorage, requests, and transfer\n.\n\n\nFor transfer, putting data into AWS is free, but youll pay on the way out. Transfer from S3 to EC2 in the \nsame region\n is free. Transfer to other regions or the Internet in general is not free.\n\n\nDeletes are free.\n\n\n\n\n\n\nS3 Reduced Redundancy and Infrequent Access:\n Most people use the Standard storage class in S3, but there are other storage classes with lower cost:\n\n\nReduced Redundancy Storage (RRS)\n has lower durability (99.99%, so just four nines). That is, theres a small chance youll lose data. For some data sets where data has value in a statistical way (losing say half a percent of your objects isnt a big deal) this is a reasonable trade-off.\n\n\nInfrequent Access (IA)\n lets you get cheaper storage in exchange for more expensive access. This is great for archives like logs you already processed, but might want to look at later. To get an idea of the cost savings when using Infrequent Access (IA), you can use this \nS3 Infrequent Access Calculator\n.\n\n\nGlacier\n is a third alternative discussed as a separate product.\n\n\nSee \nthe comparison table\n.\n\n\n\n\n\n\nPerformance:\n Maximizing S3 performance means improving overall throughput in terms of bandwidth and number of operations per second.\n\n\nS3 is highly scalable, so in principle you can get arbitrarily high throughput. (A good example of this is \nS3DistCp\n.)\n\n\nBut usually you are constrained by the pipe between the source and S3 and/or the level of concurrency of operations.\n\n\nThroughput is of course highest from within AWS to S3, and between EC2 instances and S3 buckets that are in the same region.\n\n\nBandwidth from EC2 depends on instance type. See the Network Performance column at \nec2instances.info\n.\n\n\nThroughput of many objects is extremely high when data is accessed in a distributed way, from many EC2 instances. Its possible to read or write objects from S3 from hundreds or thousands of instances at once.\n\n\nHowever, throughput is very limited when objects accessed sequentially from a single instance. Individual operations take many milliseconds, and bandwidth to and from instances is limited.\n\n\nTherefore, to perform large numbers of operations, its necessary to use multiple worker threads and connections on individual instances, and for larger jobs, multiple EC2 instances as well.\n\n\nMulti-part uploads:\n For large objects you want to take advantage of the multi-part uploading capabilities (starting with minimum chunk sizes of 5 MB).\n\n\nLarge downloads:\n Also you can download chunks of a single large object in parallel by exploiting the HTTP GET range-header capability.\n\n\nList pagination:\n Listing contents happens at 1000 responses per request, so for buckets with many millions of objects listings will take time.\n\n\nKey prefixes:\n In addition, latency on operations is \nhighly dependent on prefix similarities among key names\n. If you have need for high volumes of operations, it is essential to consider naming schemes with more randomness early in the key name (first 6 or 8 characters) in order to avoid hot spots.\n\n\nWe list this as a major gotcha since its often painful to do large-scale renames.\n\n\nNote that sadly, the advice about random key names goes against having a consistent layout with common prefixes to manage data lifecycles in an automated way.\n\n\n\n\n\n\nFor data outside AWS, \nDirectConnect\n and \nS3 Transfer Acceleration\n can help. For S3 Transfer Acceleration, you \npay\n about the equivalent of 1-2 months of storage for the transfer in either direction for using nearer endpoints.\n\n\n\n\n\n\nCommand-line applications:\n There are a few ways to use S3 from the command line:\n\n\nOriginally, \ns3cmd\n was the best tool for the job. Its still used heavily by many.\n\n\nThe regular \naws\n command-line interface now supports S3 well, and is useful for most situations.\n\n\ns4cmd\n is a replacement, with greater emphasis on performance via multi-threading, which is helpful for large files and large sets of files, and also offers Unix-like globbing support.\n\n\n\n\n\n\nGUI applications:\n You may prefer a GUI, or wish to support GUI access for less technical users. Some options:\n\n\nThe \nAWS Console\n does offer a graphical way to use S3. Use caution telling non-technical people to use it, however, since without tight permissions, it offers access to many other AWS features.\n\n\nTransmit\n is a good option on OS X for basic use cases. Uses legacy AWS2 signatures for authentication and is missing multipart upload support.\n\n\nCyberduck\n is a good option on OS X and Windows with support for multipart uploads, ACLs, versioning, lifecycle configuration, storage classes and server side encryption (SSE-S3 and SSE-KMS).\n\n\n\n\n\n\nS3 and CloudFront:\n S3 is tightly integrated with the CloudFront CDN. See the CloudFront section for more information, as well as \nS3 transfer acceleration\n.\n\n\nStatic website hosting:\n\n\nS3 has a \nstatic website hosting option\n that is simply a setting that enables configurable HTTP index and error pages and \nHTTP redirect support\n to \npublic content\n in S3. Its a simple way to host static assets or a fully static website.\n\n\nConsider using CloudFront in front of most or all assets:\n\n\nLike any CDN, CloudFront improves performance significantly.\n\n\nSSL is only supported on the built-in amazonaws.com domain for S3. S3 supports serving these sites through a \ncustom domain\n, but \nnot over SSL on a custom domain\n. However, \nCloudFront allows you to serve a custom domain over https\n. Amazon provides free SNI SSL/TLS certificates via Amazon Certificate Manager. \nSNI does not work on very outdated browsers/operating systems\n. Alternatively, you can provide your own certificate to use on CloudFront to support all browsers/operating systems.\n\n\nIf you are including resources across domains, such as fonts inside CSS files, you may need to \nconfigure CORS\n for the bucket serving those resources.\n\n\nSince pretty much everything is moving to SSL nowadays, and you likely want control over the domain, you probably want to set up CloudFront with your own certificate in front of S3 (and to ignore the \nAWS example on this\n as it is non-SSL only).\n\n\nThat said, if you do, youll need to think through invalidation or updates on CloudFront. You may wish to \ninclude versions or hashes in filenames\n so invalidation is not necessary.\n\n\n\n\n\n\n\n\n\n\nPermissions:\n\n\nIts important to manage permissions sensibly on S3 if you have data sensitivities, as fixing this later can be a difficult task if you have a lot of assets and internal users.\n\n\nDo create new buckets if you have different data sensitivities, as this is much less error prone than complex permissions rules.\n\n\nIf data is for administrators only, like log data, put it in a bucket that only administrators can access.\n\n\nLimit individual user (or IAM role) access to S3 to the minimal required and catalog the approved locations. Otherwise, S3 tends to become the dumping ground where people put data to random locations that are not cleaned up for years, costing you big bucks.\n\n\n\n\n\n\nData lifecycles:\n\n\nWhen managing data, the understanding the lifecycle of the data is as important as understanding the data itself. When putting data into a bucket, think about its lifecycle  its end of life, not just its beginning.\n\n\nIn general, data with different expiration policies should be stored under separate prefixes at the top level. For example, some voluminous logs might need to be deleted automatically monthly, while other data is critical and should never be deleted. Having the former in a separate bucket or at least a separate folder is wise.\n\n\nThinking about this up front will save you pain. Its very hard to clean up large collections of files created by many engineers with varying lifecycles and no coherent organization.\n\n\nAlternatively you can set a lifecycle policy to archive old data to Glacier. \nBe careful\n with archiving large numbers of small objects to Glacier, since it may actually cost more.\n\n\nThere is also a storage class called \nInfrequent Access\n that has the same durability as Standard S3, but is discounted per GB. It is suitable for objects that are infrequently accessed.\n\n\n\n\n\n\nData consistency:\n Understanding \ndata consistency\n is critical for any use of S3 where there are multiple producers and consumers of data.\n\n\nCreation of individual objects in S3 is atomic. Youll never upload a file and have another client see only half the file.\n\n\nAlso, if you create a new object, youll be able to read it instantly, which is called \nread-after-write consistency\n.\n\n\nWell, with the additional caveat that if you do a read on an object before it exists, then create it, \nyou get eventual consistency\n (not read-after-write).\n\n\n\n\n\n\nIf you overwrite or delete an object, youre only guaranteed eventual consistency.\n\n\nNote that \nuntil 2015\n, 'us-standard' region had had a weaker eventual consistency model, and the other (newer) regions were read-after-write. This was finally corrected  but watch for many old blogs mentioning this!\n\n\nIn practice, eventual consistency usually means within seconds, but expect rare cases of minutes or \nhours\n.\n\n\n\n\n\n\nS3 as a filesystem:\n\n\nIn general S3s APIs have inherent limitations that make S3 hard to use directly as a POSIX-style filesystem while still preserving S3s own object format. For example, appending to a file requires rewriting, which cripples performance, and atomic rename of directories, mutual exclusion on opening files, and hardlinks are impossible.\n\n\ns3fs\n is a FUSE filesystem that goes ahead and tries anyway, but it has performance limitations and surprises for these reasons.\n\n\nRiofs\n (C) and \nGoofys\n (Go) are more recent efforts that attempt adopt a different data storage format to address those issues, and so are likely improvements on s3fs.\n\n\nS3QL\n (\ndiscussion\n) is a Python implementation that offers data de-duplication, snap-shotting, and encryption, but only one client at a time.\n\n\nObjectiveFS\n (\ndiscussion\n) is a commercial solution that supports filesystem features and concurrent clients.\n\n\n\n\n\n\nIf you are primarily using a VPC, consider setting up a \nVPC Endpoint\n for S3 in order to allow your VPC-hosted resources to easily access it without the need for extra network configuration or hops.\n\n\nCross-region replication:\n S3 has \na feature\n for replicating a bucket between one region and another. Note that S3 is already highly replicated within one region, so usually this isnt necessary for durability, but it could be useful for compliance (geographically distributed data storage), lower latency, or as a strategy to reduce region-to-region bandwidth costs by mirroring heavily used data in a second region.\n\n\nIPv4 vs IPv6:\n For a long time S3 only supported IPv4 at the default endpoint \nhttps://BUCKET.s3.amazonaws.com\n. However, \nas of Aug 11, 2016\n it now supports both IPv4 \n IPv6! To use both, you have to \nenable dualstack\n either in your preferred API client or by directly using this url scheme \nhttps://BUCKET.s3.dualstack.REGION.amazonaws.com\n. This extends to S3 Transfer Acceleration as well.\n\n\nS3 event notifications:\n S3 can be configured to send an \nSNS notification\n, \nSQS message\n, or \nAWS Lambda function\n on \nbucket events\n.", 
            "title": "Tips"
        }, 
        {
            "location": "/SES/basics/", 
            "text": "Homepage\n  \nDocumentation\n  \nFAQ\n  \nPricing\n\n\nSES\n (or Simple Email Service) is a service that exposes SMTP endpoints for your application to directly integrate with.", 
            "title": "Basics"
        }, 
        {
            "location": "/SES/gotchas-and-limitations/", 
            "text": "Internet Access:\n SES SMTP endpoints are on the Internet and will not be accessible from a location without Internet access (e.g. a private subnet without NAT gateway route in the routing table). In such a case, set up an SMTP relay instance in a subnet with Internet access and configure your application to send emails to this SMTP relay instance rather than SES. The relay should have a \nforwarding rule to send all emails to SES\n). If you are using a proxy instead of a NAT, confirm that your proxy service supports SMTP.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/SES/tips/", 
            "text": "Bounce Handling:\n Make sure you handle this early enough. Your ability to send emails can be removed if SES sees \ntoo many bounces\n.\n\n\nCredentials:\n Many developers get confused between \nSES credentials\n and AWS API keys. Make sure to enter \nSMTP credentials\n while using the SMTP APIs.", 
            "title": "Tips"
        }, 
        {
            "location": "/Security And IAM/security-and-iam-basics/", 
            "text": "IAM \nHomepage\n  \nUser guide\n  \nFAQ\n\n\nThe \nAWS Security Blog\n is one of the best sources of news and information on AWS security.\n\n\nIAM\n is the service you use to manage accounts and permissioning for AWS.\n\n\nManaging security and access control with AWS is critical, so every AWS administrator needs to use and understand IAM, at least at a basic level.\n\n\nIAM identities\n include users (people or services that are using AWS), groups (containers for sets of users and their permissions), and roles (containers for permissions assigned to AWS service instances). \nPermissions\n for these identities are governed by \npolicies\n You can use AWS pre-defined policies or custom policies that you create.\n\n\nIAM manages various kinds of authentication, for both users and for software services that may need to authenticate with AWS, including:\n\n\nPasswords\n to log into the console. These are a username and password for real users.\n\n\nAccess keys\n, which you may use with command-line tools. These are two strings, one the id, which is an upper-case alphabetic string of the form 'AXXXXXXXXXXXXXXXXXXX', and the other is the secret, which is a 40-character mixed-case base64-style string. These are often set up for services, not just users.\n\n\nAccess keys that start with AKIA are normal keys. Access keys that start with ASIA are session/temporary keys from STS, and will require an additional \"SessionToken\" parameter to be sent along with the id and secret.\n\n\n\n\n\n\nMulti-factor authentication (MFA)\n, which is the highly recommended practice of using a keychain fob or smartphone app as a second layer of protection for user authentication.\n\n\n\n\n\n\nIAM allows complex and fine-grained control of permissions, dividing users into groups, assigning permissions to roles, and so on. There is a \npolicy language\n that can be used to customize security policies in a fine-grained way.\n\n\nThe policy language has a complex and error-prone JSON syntax thats quite confusing, so unless you are an expert, it is wise to base yours off trusted examples or AWS own pre-defined \nmanaged policies\n.\n\n\n\n\n\n\nAt the beginning, IAM policy may be very simple, but for large systems, it will grow in complexity, and need to be managed with care.\n\n\nMake sure one person (perhaps with a backup) in your organization is formally assigned ownership of managing IAM policies, make sure every administrator works with that person to have changes reviewed. This goes a long way to avoiding accidental and serious misconfigurations.\n\n\n\n\n\n\nIt is best to give each user or service the minimum privileges needed to perform their duties. This is the \nprinciple of least privilege\n, one of the foundations of good security. Organize all IAM users and groups according to levels of access they need.\n\n\nIAM has the \npermission hierarchy\n of:\n\n\nExplicit deny: The most restrictive policy wins.\n\n\nExplicit allow: Access permissions to any resource has to be explicitly given.\n\n\nImplicit deny: All permissions are implicitly denied by default.\n\n\n\n\n\n\nYou can test policy permissions via the AWS IAM \npolicy simulator tool tool\n. This is particularly useful if you write custom policies.", 
            "title": "Security and iam basics"
        }, 
        {
            "location": "/Security And IAM/security-and-iam-gotchas-and-limitations/", 
            "text": "Dont share user credentials:\n Its remarkably common for first-time AWS users to create one account and one set of credentials (access key or password), and then use them for a while, sharing among engineers and others within a company. This is easy. But \ndont do this\n. This is an insecure practice for many reasons, but in particular, if you do, you will have reduced ability to revoke credentials on a per-user or per-service basis (for example, if an employee leaves or a key is compromised), which can lead to serious complications.\n\n\nInstance metadata throttling:\n The \ninstance metadata service\n has rate limiting on API calls. If you deploy IAM roles widely (as you should!) and have lots of services, you may hit global account limits easily.\n\n\nOne solution is to have code or scripts cache and reuse the credentials locally for a short period (say 2 minutes). For example, they can be put into the ~/.aws/credentials file but must also be refreshed automatically.\n\n\nBut be careful not to cache credentials for too long, as \nthey expire\n. (Note the other \ndynamic metadata\n also changes over time and should not be cached a long time, either.)\n\n\n\n\n\n\nSome IAM operations are slower than other API calls (many seconds), since AWS needs to propagate these globally across regions.\n\n\nThe uptime of IAMs API has historically been lower than that of the instance metadata API. Be wary of incorporating a dependency on IAMs API into critical paths or subsystems  for example, if you validate a users IAM group membership when they log into an instance and arent careful about precaching group membership or maintaining a back door, you might end up locking users out altogether when the API isnt available.\n\n\nDon't check in AWS credentials or secrets to a git repository.\n There are bots that scan GitHub looking for credentials. Use scripts or tools, such as \ngit-secrets\n to prevent anyone on your team from checking in sensitive information to your git repositories.", 
            "title": "Security and iam gotchas and limitations"
        }, 
        {
            "location": "/Security And IAM/security-and-iam-tips/", 
            "text": "Use IAM to create individual user accounts and \nuse IAM accounts for all users from the beginning\n. This is slightly more work, but not that much.\n\n\nThat way, you define different users, and groups with different levels of privilege (if you want, choose from Amazons default suggestions, of administrator, power user, etc.).\n\n\nThis allows credential revocation, which is critical in some situations. If an employee leaves, or a key is compromised, you can revoke credentials with little effort.\n\n\nYou can set up \nActive Directory federation\n to use organizational accounts in AWS.\n\n\n\n\n\n\nDo NOT use the \nIAM Root User account\n other than when you initially create your account.  Create custom IAM users and/or roles and use those for your applications instead.\n\n\nEnable \nMFA\n on your account.\n\n\nYou should always use MFA, and the sooner the better  enabling it when you already have many users is extra work.\n\n\nUnfortunately it cant be enforced in software, so an administrative policy has to be established.\n\n\nMost users can use the Google Authenticator app (on \niOS\n or \nAndroid\n) to support two-factor authentication. For the root account, consider a hardware fob.\n\n\n\n\n\n\nTurn on CloudTrail:\n One of the first things you should do is \nenable CloudTrail\n. Even if you are not a security hawk, there is little reason not to do this from the beginning, so you have data on what has been happening in your AWS account should you need that information. Youll likely also want to set up a \nlog management service\n to search and access these logs.\n\n\nUse IAM roles for EC2:\n Rather than assign IAM users to applications like services and then sharing the sensitive credentials, \ndefine and assign roles to EC2 instances\n and have applications retrieve credentials from the \ninstance metadata\n.\n\n\nAssign IAM roles by realm  for example, to development, staging, and production. If youre setting up a role, it should be tied to a specific realm so you have clean separation. This prevents, for example, a development instance from connecting to a production database.\n\n\nBest practices:\n AWS \nlist of best practices\n is worth reading in full up front.\n\n\nMultiple accounts:\n Decide on whether you want to use multiple AWS accounts and \nresearch\n how to organize access across them. Factors to consider:\n\n\nNumber of users\n\n\nImportance of isolation\n\n\nResource Limits\n\n\nPermission granularity\n\n\nSecurity\n\n\nAPI Limits\n\n\n\n\n\n\nRegulatory issues\n\n\nWorkload\n\n\nSize of infrastructure\n\n\nCost of multi-account overhead: Internal AWS service management tools may need to be custom built or adapted.\n\n\nIt can help to use separate AWS accounts for independent parts of your infrastructure if you expect a high rate of AWS API calls, since AWS \nthrottles calls\n at the AWS account level.\n\n\n\n\n\n\nInspector\n is an automated security assessment service from AWS that helps identify common security risks. This allows validation that you adhere to certain security practices and may help with compliance.\n\n\nTrusted Advisor\n addresses a variety of best practices, but also offers some basic security checks around IAM usage, security group configurations, and MFA. At paid support tiers, Trusted Advisor exposes additional checks around other areas, such as reserved instance optimization.\n\n\nUse KMS for managing keys\n: AWS offers \nKMS\n for securely managing encryption keys, which is usually a far better option than handling key security yourself. See \nbelow\n.\n\n\nAWS WAF\n is a web application firewall to help you protect your applications from common attack patterns.\n\n\nSecurity auditing:\n\n\nSecurity Monkey\n is an open source tool that is designed to assist with security audits.\n\n\nScout2\n is an open source tool that uses AWS APIs to assess an environments security posture. Scout2 is stable and actively maintained.\n\n\nExport and audit security settings:\n You can audit security policies simply by exporting settings using AWS APIs, e.g. using a Boto script like \nSecConfig.py\n (from \nthis 2013 talk\n) and then reviewing and monitoring changes manually or automatically.", 
            "title": "Security and iam tips"
        }, 
        {
            "location": "/VPC Network Security/basics/", 
            "text": "Homepage\n  \nUser guide\n  \nFAQ\n  \nSecurity groups\n  \nPricing\n\n\nVPC\n (Virtual Private Cloud) is the virtualized networking layer of your AWS systems.\n\n\nMost AWS users should have a basic understanding of VPC concepts, but few need to get into all the details. VPC configurations can be trivial or extremely complex, depending on the extent of your network and security needs.\n\n\nAll modern AWS accounts (those created \nafter 2013-12-04\n) are EC2-VPC accounts that support VPCs, and all instances will be in a default VPC. Older accounts may still be using EC2-Classic mode. Some features dont work without VPCs, so you probably will want to \nmigrate\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/VPC Network Security/gotchas-and-limitations/", 
            "text": "Security groups are not shared across data centers, so if you have infrastructure in multiple data centers, you should make sure your configuration/deployment tools take that into account.\n\n\nBe careful when choosing your VPC IP CIDR block: If you are going to need to make use of \nClassicLink\n, make sure that your private IP range \ndoesnt overlap\n with that of EC2 Classic.\n\n\nIf you are going to peer VPCs, carefully consider the cost of \ndata transfer between VPCs\n, since for some workloads and integrations, this can be prohibitively expensive.\n\n\nNew RDS instances require a \nsubnet group\n within your VPC. If youre using the \ndefault VPC\n this isnt a concern, it will contain a subnet for each availability zone in your region. However, if youre creating your own VPC and plan on using RDS, make sure you have at least two subnets within the VPC to act as the subnet group.\n\n\nIf you delete the default VPC, the only way to create another VPC marked as default is to contact AWS technical support. See this \nnote\n in the documentation.\n\n\nBe careful with VPC VPN credentials! If lost or compromised, the VPN endpoint must be deleted and recreated. See the instructions for \nReplacing Compromised Credentials\n.\n\n\nSecurity Groups and Route Tables apply entries separately for IPv4 and IPv6, so one must ensure they add entries for both protocols accordingly.\n\n\nAt launch, IPv6 for VPC is only available in the Ohio (us-east-2) region, though the launch announcement makes mention that other regions will see the same in the future.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/VPC Network Security/tips/", 
            "text": "Security groups\n are your first line of defense for your servers. Be extremely restrictive of what ports are open to all incoming connections. In general, if you use CLBs, ALBs or other load balancing, the only ports that need to be open to incoming traffic would be port 22 and whatever port your application uses. Security groups access policy is 'deny by default'.\n\n\nPort hygiene:\n A good habit is to pick unique ports within an unusual range for each different kind of production service. For example, your web frontend might use 3010, your backend services 3020 and 3021, and your Postgres instances the usual 5432. Then make sure you have fine-grained security groups for each set of servers. This makes you disciplined about listing out your services, but also is more error-proof. For example, should you accidentally have an extra Apache server running on the default port 80 on a backend server, it will not be exposed.\n\n\nMigrating from Classic\n: For migrating from older EC2-Classic deployments to modern EC2-VPC setup, \nthis article\n may be of help.\n\n\nYou can \nmigrate Elastic IPs between EC2-Classic and EC2-VPC\n.\n\n\n\n\n\n\nFor basic AWS use, one default VPC may be sufficient. But as you scale up, you should consider mapping out network topology more thoroughly. A good overview of best practices is \nhere\n.\n\n\nConsider controlling access to you private AWS resources through a \nVPN\n.\n\n\nYou get better visibility into and control of connection and connection attempts.\n\n\nYou expose a smaller surface area for attack compared to exposing separate (potentially authenticated) services over the public internet.\n\n\ne.g. A bug in the YAML parser used by the Ruby on Rails admin site is much less serious when the admin site is only visible to the private network and accessed through VPN.\n\n\n\n\n\n\nAnother common pattern (especially as deployments get larger, security or regulatory requirements get more stringent, or team sizes increase) is to provide a \nbastion host\n behind a VPN through which all SSH connections need to transit.\n\n\n\n\n\n\nConsider using other security groups as sources for security group rules instead of using CIDRs  that way, all hosts in the source security group and only hosts in that security group are allowed access. This is a much more dynamic and secure way of managing security group rules.\n\n\nVPC Flow Logs\n allow you to monitor the network traffic to, from, and within your VPC. Logs are stored in CloudWatch Logs groups, and can be used for security monitoring (with third party tools), performance evaluation, and forensic investigation.\n\n\nSee the \nVPC Flow Logs User Guide\n for basic information.\n\n\nSee the \nflowlogs-reader\n CLI tool and Python library to retrieve and work with VPC Flow Logs.\n\n\n\n\n\n\nIPv6\n \nis available in VPC.\n Along with this announcement came the introduction of the Egress-Only Internet Gateway. In cases where one would use NAT Gateways to enable egress-only traffic for their VPC in IPv4, one can use an Egress-Only Internet Gateway for the same purpose in IPv6.\n\n\n\n\nIPv6\n \nis available in VPC\n. Along with this announcement came the introduction of the \nEgress-Only Internet Gateway\n. In cases where one would use NAT Gateways to enable egress-only traffic for their VPC in IPv4, one can use an Egress-Only Internet Gateway for the same purpose in IPv6.\n\n\n\n\n\n\nAmazon provides an IPv6 CIDR block for your VPC at your request - at present you cannot implement your own IPv6 block if you happen to own one already.\n\n\n\n\nNew and existing VPCs can both use IPv6. Existing VPCs will need to be configured to have an IPv6 CIDR block associated with them, just as new VPCs do.", 
            "title": "Tips"
        }, 
        {
            "location": "/WAF/basics/", 
            "text": "Homepage\n  \nDocumentation\n  \nFAQ\n  \nPricing\n\n\nWAF (Web Application Firewall) is used in conjunction with the CloudFront and ALB services to inspect and block/allow web requests based on user-configurable conditions.\n\n\nHTTPS and HTTP requests are supported with this service.\n\n\nWAF's strength is in detecting malicious activity based on pattern-matching inputs for attacks such as SQL injections, XSS, etc.\n\n\nWAF supports inspection of requests \nreceived through both IPv6 and IPv4\n.", 
            "title": "Basics"
        }, 
        {
            "location": "/WAF/gotchas-and-limitations/", 
            "text": "As of December 2016, WAF is available in the US East (Northern Virginia), US West (Oregon), Asia Pacific (Tokyo) and EU (Ireland) regions.", 
            "title": "Gotchas and limitations"
        }, 
        {
            "location": "/WAF/tips/", 
            "text": "Getting a WAF API call history can be done through CloudTrail. This is enabled through the CloudTrail console.", 
            "title": "Tips"
        }
    ]
}